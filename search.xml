<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>VINS-Mono——视觉前段处理</title>
    <url>/2019/10/23/VINS-Mono%E2%80%94%E2%80%94%E8%A7%86%E8%A7%89%E5%89%8D%E6%AE%B5%E5%A4%84%E7%90%86/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="论文："><a href="#论文：" class="headerlink" title="论文："></a>论文：</h1><p>每当进入新的图像，都会使用KLT稀疏光流法进行跟踪，同时提取100-300个角点信息，我的理解是角点是用来建立图像，光流跟踪是用来快速定位。同时在这里还进行了关键帧的选取（注意这一过程在代码中是由vins_estimate文件中实现的），主要是两个剔除关键帧的策略，分别是平均视差法和跟踪质量法。平均视差法：如果当前帧的和上一个关键帧跟踪点的平均视差超出了一个设定的阈值，就将当前帧设为关键帧。这里有一个问题，就是旋转和平移都会产生视差（不只是平移哦），当出现纯旋转的时候特征点无法被三角化，无法计算出旋转值，也就无法计算跟踪点间的平均视差，为了解决这一问题，采用短时的陀螺仪观测值来补偿旋转，从而计算出视差，这一过程只应用到平均视差的计算，不会影响真实的旋转结果。</p><a id="more"></a>
<h1 id="流程图"><a href="#流程图" class="headerlink" title="流程图"></a>流程图</h1><p><img alt="在这里插入图片描述" data-src="https://img-blog.csdnimg.cn/20191023190400778.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjkwNTE0MQ==,size_16,color_FFFFFF,t_70"></p>
<h1 id="节点概览"><a href="#节点概览" class="headerlink" title="节点概览"></a>节点概览</h1><p>该节点的功能是：接收图像数据，进行角点提取和光流跟踪，输出跟踪的特征点（角点）。feature_tracker_node的消息订阅发布如下表：<br><img alt="在这里插入图片描述" data-src="https://img-blog.csdnimg.cn/20191023221410832.png"></p>
<h1 id="代码细节"><a href="#代码细节" class="headerlink" title="代码细节"></a>代码细节</h1><p>主要负责图像角点提取和光流跟踪，主要是三个源程序，分别是feature_tracker、feature_tracker_node以及parameters。feature_tracker_node是特征跟踪线程的系统入口，feature_tracker是特征跟踪算法的具体实现，parameters是设备等参数的读取和存放。</p>
<h2 id="程序入口main-函数"><a href="#程序入口main-函数" class="headerlink" title="程序入口main()函数"></a>程序入口main()函数</h2><ul>
<li>1、ros初始化和设置句柄，设置logger级别</li>
</ul>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">ros::init(argc, argv, <span class="string">"feature_tracker"</span>);</span><br><span class="line">ros::<span class="function">NodeHandle <span class="title">n</span><span class="params">(<span class="string">"~"</span>)</span></span>;</span><br><span class="line">ros::console::set_logger_level(ROSCONSOLE_DEFAULT_NAME, ros::console::levels::Info);</span><br></pre></td></tr></table></figure>
<ul>
<li>2、readParameters(n);读取参数，如config-&gt;euroc-&gt;euroc_config.yaml中的一些配置参数</li>
</ul>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">readParameters(n);</span><br></pre></td></tr></table></figure>
<ul>
<li>3、读取每个相机实例读取对应的相机内参，NUM_OF_CAM=1为单目</li>
</ul>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; NUM_OF_CAM; i++) </span><br><span class="line">       trackerData[i].readIntrinsicParameter(CAM_NAMES[i]);</span><br></pre></td></tr></table></figure>
<p>节点在启动时会先读取相机内参，根据config_file文件中model_type的值决定采用何种相机模型，并创建相应模型的对象指针，读取在该模型下需要的参数<br><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">void</span> FeatureTracker::readIntrinsicParameter(<span class="keyword">const</span> <span class="built_in">string</span> &amp;calib_file)</span><br><span class="line">&#123;</span><br><span class="line">    ROS_INFO(<span class="string">"reading paramerter of camera %s"</span>, calib_file.c_str());</span><br><span class="line">    m_camera = CameraFactory::instance()-&gt;generateCameraFromYamlFile(calib_file);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">CameraPtr</span><br><span class="line">CameraFactory::generateCameraFromYamlFile(<span class="keyword">const</span> <span class="built_in">std</span>::<span class="built_in">string</span>&amp; filename)</span><br><span class="line">&#123;</span><br><span class="line">    cv::<span class="function">FileStorage <span class="title">fs</span><span class="params">(filename, cv::FileStorage::READ)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (!fs.isOpened())</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> CameraPtr();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    Camera::ModelType modelType = Camera::MEI;</span><br><span class="line">    <span class="keyword">if</span> (!fs[<span class="string">"model_type"</span>].isNone())</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">std</span>::<span class="built_in">string</span> sModelType;</span><br><span class="line">        fs[<span class="string">"model_type"</span>] &gt;&gt; sModelType;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//卡特鱼眼相机</span></span><br><span class="line">        <span class="keyword">if</span> (boost::iequals(sModelType, <span class="string">"kannala_brandt"</span>))</span><br><span class="line">        &#123;</span><br><span class="line">            modelType = Camera::KANNALA_BRANDT;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> (boost::iequals(sModelType, <span class="string">"mei"</span>))</span><br><span class="line">        &#123;</span><br><span class="line">            modelType = Camera::MEI;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> (boost::iequals(sModelType, <span class="string">"scaramuzza"</span>))</span><br><span class="line">        &#123;</span><br><span class="line">            modelType = Camera::SCARAMUZZA;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> (boost::iequals(sModelType, <span class="string">"pinhole"</span>))</span><br><span class="line">        &#123;</span><br><span class="line">            modelType = Camera::PINHOLE;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">        &#123;</span><br><span class="line">            <span class="built_in">std</span>::<span class="built_in">cerr</span> &lt;&lt; <span class="string">"# ERROR: Unknown camera model: "</span> &lt;&lt; sModelType &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">            <span class="keyword">return</span> CameraPtr();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">switch</span> (modelType)</span><br><span class="line">    &#123;</span><br><span class="line">    <span class="keyword">case</span> Camera::KANNALA_BRANDT:</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="function">EquidistantCameraPtr <span class="title">camera</span><span class="params">(<span class="keyword">new</span> EquidistantCamera)</span></span>;</span><br><span class="line"></span><br><span class="line">        EquidistantCamera::Parameters params = camera-&gt;getParameters();</span><br><span class="line">        params.readFromYamlFile(filename);</span><br><span class="line">        camera-&gt;setParameters(params);</span><br><span class="line">        <span class="keyword">return</span> camera;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">case</span> Camera::PINHOLE:</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="function">PinholeCameraPtr <span class="title">camera</span><span class="params">(<span class="keyword">new</span> PinholeCamera)</span></span>;</span><br><span class="line"></span><br><span class="line">        PinholeCamera::Parameters params = camera-&gt;getParameters();</span><br><span class="line">        params.readFromYamlFile(filename);</span><br><span class="line">        camera-&gt;setParameters(params);</span><br><span class="line">        <span class="keyword">return</span> camera;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">case</span> Camera::SCARAMUZZA:</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="function">OCAMCameraPtr <span class="title">camera</span><span class="params">(<span class="keyword">new</span> OCAMCamera)</span></span>;</span><br><span class="line"></span><br><span class="line">        OCAMCamera::Parameters params = camera-&gt;getParameters();</span><br><span class="line">        params.readFromYamlFile(filename);</span><br><span class="line">        camera-&gt;setParameters(params);</span><br><span class="line">        <span class="keyword">return</span> camera;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">case</span> Camera::MEI:</span><br><span class="line">    <span class="keyword">default</span>:</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="function">CataCameraPtr <span class="title">camera</span><span class="params">(<span class="keyword">new</span> CataCamera)</span></span>;</span><br><span class="line"></span><br><span class="line">        CataCamera::Parameters params = camera-&gt;getParameters();</span><br><span class="line">        params.readFromYamlFile(filename);</span><br><span class="line">        camera-&gt;setParameters(params);</span><br><span class="line">        <span class="keyword">return</span> camera;</span><br><span class="line">    &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> CameraPtr();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>4、判断是否加入鱼眼mask来去除边缘噪声</li>
</ul>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span>(FISHEYE)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; NUM_OF_CAM; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment">//FISHEYE_MASK  fisheye_mask.jpg 路径</span></span><br><span class="line">        trackerData[i].fisheye_mask = cv::imread(FISHEYE_MASK, <span class="number">0</span>);</span><br><span class="line">        <span class="keyword">if</span>(!trackerData[i].fisheye_mask.data)</span><br><span class="line">        &#123;</span><br><span class="line">            ROS_INFO(<span class="string">"load mask fail"</span>);</span><br><span class="line">            ROS_BREAK();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            ROS_INFO(<span class="string">"load mask success"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>5、订阅话题IMAGE_TOPIC(如/cam0/image_raw)，执行回调函数img_callback，对新来的图像进行特征点追踪、处理和发布<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">ros::Subscriber sub_img = n.subscribe(IMAGE_TOPIC, <span class="number">100</span>, img_callback);</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>单目时：FeatureTracker::readImage() 函数读取图像数据进行处理<br>单目i=0<br><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">trackerData[i].readImage(ptr-&gt;image.rowRange(ROW * i, ROW * (i + <span class="number">1</span>)), img_msg-&gt;header.stamp.toSec());</span><br></pre></td></tr></table></figure></p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">void</span> FeatureTracker::readImage(<span class="keyword">const</span> cv::Mat &amp;_img, <span class="keyword">double</span> _cur_time)</span><br><span class="line">&#123;</span><br><span class="line">    cv::Mat img;</span><br><span class="line">    TicToc t_r;</span><br><span class="line">    cur_time = _cur_time;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (EQUALIZE)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment">// Contrast Limited AHE(Adaptive histogram equalization)</span></span><br><span class="line">        cv::Ptr&lt;cv::CLAHE&gt; clahe = cv::createCLAHE(<span class="number">3.0</span>, cv::Size(<span class="number">8</span>, <span class="number">8</span>));</span><br><span class="line">        TicToc t_c;</span><br><span class="line">        clahe-&gt;apply(_img, img);</span><br><span class="line">        ROS_DEBUG(<span class="string">"CLAHE costs: %fms"</span>, t_c.toc());</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        img = _img;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (forw_img.empty())</span><br><span class="line">        prev_img = cur_img = forw_img = img;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        forw_img = img;</span><br><span class="line"></span><br><span class="line">    forw_pts.clear();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (cur_pts.size() &gt; <span class="number">0</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        TicToc t_o;</span><br><span class="line">        <span class="built_in">vector</span>&lt;uchar&gt; status;</span><br><span class="line">        <span class="built_in">vector</span>&lt;<span class="keyword">float</span>&gt; err;</span><br><span class="line">        cv::calcOpticalFlowPyrLK(cur_img, forw_img, cur_pts, forw_pts, status, err, cv::Size(<span class="number">21</span>, <span class="number">21</span>), <span class="number">3</span>);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="keyword">int</span>(forw_pts.size()); i++)</span><br><span class="line">            <span class="keyword">if</span> (status[i] &amp;&amp; !inBorder(forw_pts[i]))</span><br><span class="line">                status[i] = <span class="number">0</span>;</span><br><span class="line">        reduceVector(prev_pts, status);</span><br><span class="line">        reduceVector(cur_pts, status);</span><br><span class="line">        reduceVector(forw_pts, status);</span><br><span class="line">        reduceVector(ids, status);</span><br><span class="line">        reduceVector(cur_un_pts, status);</span><br><span class="line">        reduceVector(track_cnt, status);</span><br><span class="line">        ROS_DEBUG(<span class="string">"temporal optical flow costs: %fms"</span>, t_o.toc());</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">auto</span> &amp;n : track_cnt)</span><br><span class="line">        n++;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (PUB_THIS_FRAME)</span><br><span class="line">    &#123;</span><br><span class="line">        rejectWithF();</span><br><span class="line">        ROS_DEBUG(<span class="string">"set mask begins"</span>);</span><br><span class="line">        TicToc t_m;</span><br><span class="line">        setMask();</span><br><span class="line">        ROS_DEBUG(<span class="string">"set mask costs %fms"</span>, t_m.toc());</span><br><span class="line"></span><br><span class="line">        ROS_DEBUG(<span class="string">"detect feature begins"</span>);</span><br><span class="line">        TicToc <span class="keyword">t_t</span>;</span><br><span class="line">        <span class="keyword">int</span> n_max_cnt = MAX_CNT - <span class="keyword">static_cast</span>&lt;<span class="keyword">int</span>&gt;(forw_pts.size());</span><br><span class="line">        <span class="keyword">if</span> (n_max_cnt &gt; <span class="number">0</span>)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">if</span>(mask.empty())</span><br><span class="line">                <span class="built_in">cout</span> &lt;&lt; <span class="string">"mask is empty "</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">            <span class="keyword">if</span> (mask.type() != CV_8UC1)</span><br><span class="line">                <span class="built_in">cout</span> &lt;&lt; <span class="string">"mask type wrong "</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">            <span class="keyword">if</span> (mask.size() != forw_img.size())</span><br><span class="line">                <span class="built_in">cout</span> &lt;&lt; <span class="string">"wrong size "</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"></span><br><span class="line">            cv::goodFeaturesToTrack(forw_img, n_pts, n_max_cnt, <span class="number">0.01</span>, MIN_DIST, mask);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            n_pts.clear();</span><br><span class="line">        ROS_DEBUG(<span class="string">"detect feature costs: %fms"</span>, <span class="keyword">t_t</span>.toc());</span><br><span class="line"></span><br><span class="line">        ROS_DEBUG(<span class="string">"add feature begins"</span>);</span><br><span class="line">        TicToc t_a;</span><br><span class="line">        addPoints();</span><br><span class="line">        ROS_DEBUG(<span class="string">"selectFeature costs: %fms"</span>, t_a.toc());</span><br><span class="line">    &#125;</span><br><span class="line">    prev_img = cur_img;</span><br><span class="line">    prev_pts = cur_pts;</span><br><span class="line">    prev_un_pts = cur_un_pts;</span><br><span class="line">    cur_img = forw_img;</span><br><span class="line">    cur_pts = forw_pts;</span><br><span class="line">    undistortedPoints();</span><br><span class="line">    prev_time = cur_time;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>6、发布feature，实例feature_points，跟踪的特征点，给后端优化用<br>发布feature_img，实例ptr，跟踪的特征点图，给RVIZ用和调试用<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">pub_img = n.advertise&lt;sensor_msgs::PointCloud&gt;(<span class="string">"feature"</span>, <span class="number">1000</span>);</span><br><span class="line">pub_match = n.advertise&lt;sensor_msgs::Image&gt;(<span class="string">"feature_img"</span>,<span class="number">1000</span>);</span><br><span class="line">pub_restart = n.advertise&lt;std_msgs::Bool&gt;(<span class="string">"restart"</span>,<span class="number">1000</span>);</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="回调函数img-callback"><a href="#回调函数img-callback" class="headerlink" title="回调函数img_callback()"></a>回调函数img_callback()</h2><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">img_callback</span><span class="params">(<span class="keyword">const</span> sensor_msgs::ImageConstPtr &amp;img_msg)</span></span></span><br></pre></td></tr></table></figure>
<p>该函数是ROS的回调函数，主要功能包括：readImage()函数对新来的图像使用光流法进行特征点跟踪，并将追踪的特征点封装成feature_points发布到pub_img的话题下，将图像封装成ptr发布在pub_match下。</p>
<ul>
<li>1、判断是否是第一帧</li>
</ul>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">//判断是否为第一帧</span></span><br><span class="line"><span class="keyword">if</span>(first_image_flag)</span><br><span class="line">&#123;</span><br><span class="line">    first_image_flag = <span class="literal">false</span>;</span><br><span class="line">    first_image_time = img_msg-&gt;header.stamp.toSec();</span><br><span class="line">    last_image_time  = img_msg-&gt;header.stamp.toSec();</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>2、判断时间间隔是否正确，有问题则restart</li>
</ul>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> (img_msg-&gt;header.stamp.toSec() - last_image_time &gt; <span class="number">1.0</span> || img_msg-&gt;header.stamp.toSec() &lt; last_image_time)</span><br><span class="line">&#123;</span><br><span class="line">    ROS_WARN(<span class="string">"image discontinue! reset the feature tracker!"</span>);</span><br><span class="line">    first_image_flag = <span class="literal">true</span>; </span><br><span class="line">    last_image_time = <span class="number">0</span>;</span><br><span class="line">    pub_count = <span class="number">1</span>;</span><br><span class="line">    std_msgs::Bool restart_flag;</span><br><span class="line">    restart_flag.data = <span class="literal">true</span>;</span><br><span class="line">    pub_restart.publish(restart_flag);</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>3、发布频率控制，并不是每读入一帧图像，就要发布特征点，通过判断间隔时间内的发布次数</li>
</ul>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">double</span> tmp_freq = <span class="number">1.0</span> * pub_count / (img_msg-&gt;header.stamp.toSec() - first_image_time);</span><br><span class="line"> <span class="keyword">if</span> (round(tmp_freq) &lt;= FREQ)</span><br><span class="line"> &#123;</span><br><span class="line">     <span class="comment">//PUB_THIS_FRAME 是否需要发布特征点</span></span><br><span class="line">     PUB_THIS_FRAME = <span class="literal">true</span>;</span><br><span class="line">     <span class="comment">// // 时间间隔内的发布频率十分接近设定频率时，更新时间间隔起始时刻，并将数据发布次数置0</span></span><br><span class="line">     <span class="keyword">if</span> (<span class="built_in">abs</span>(tmp_freq - FREQ) &lt; <span class="number">0.01</span> * FREQ)</span><br><span class="line">     &#123;</span><br><span class="line">         first_image_time = img_msg-&gt;header.stamp.toSec();</span><br><span class="line">         pub_count = <span class="number">0</span>;</span><br><span class="line">     &#125;</span><br><span class="line"> &#125;</span><br><span class="line"> <span class="keyword">else</span></span><br><span class="line">     PUB_THIS_FRAME = <span class="literal">false</span>;</span><br></pre></td></tr></table></figure>
<ul>
<li>4、将图像编码8UC1转换为mono8</li>
</ul>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">cv_bridge::CvImageConstPtr ptr;</span><br><span class="line"><span class="keyword">if</span> (img_msg-&gt;encoding == <span class="string">"8UC1"</span>)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="comment">//将图像编码8UC1转换为mono8</span></span><br><span class="line">    sensor_msgs::Image img;</span><br><span class="line">    img.header = img_msg-&gt;header;</span><br><span class="line">    img.height = img_msg-&gt;height;</span><br><span class="line">    img.width = img_msg-&gt;width;</span><br><span class="line">    img.is_bigendian = img_msg-&gt;is_bigendian;</span><br><span class="line">    img.step = img_msg-&gt;step;</span><br><span class="line">    img.data = img_msg-&gt;data;</span><br><span class="line">    img.encoding = <span class="string">"mono8"</span>;</span><br><span class="line">    ptr = cv_bridge::toCvCopy(img, sensor_msgs::image_encodings::MONO8);</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">    ptr = cv_bridge::toCvCopy(img_msg, sensor_msgs::image_encodings::MONO8);</span><br></pre></td></tr></table></figure>
<ul>
<li>5、单目时：FeatureTracker::readImage() 函数读取图像数据进行处理</li>
</ul>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; NUM_OF_CAM; i++)</span><br><span class="line">&#123;</span><br><span class="line">    ROS_DEBUG(<span class="string">"processing camera %d"</span>, i);</span><br><span class="line">    <span class="comment">//双目跟踪则为1,单目false</span></span><br><span class="line">    <span class="keyword">if</span> (i != <span class="number">1</span> || !STEREO_TRACK) <span class="comment">// 针对单目相机读入图像，进入KLT跟踪阶段</span></span><br><span class="line">        <span class="comment">//ROW = fsSettings["image_height"]  rowRange和colRange函数可以获取某些范围内行或列的指针</span></span><br><span class="line">        trackerData[i].readImage(ptr-&gt;image.rowRange(ROW * i, ROW * (i + <span class="number">1</span>)), img_msg-&gt;header.stamp.toSec());</span><br></pre></td></tr></table></figure>
<ul>
<li>6、更新特征点ID</li>
</ul>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">    <span class="keyword">for</span> (<span class="keyword">unsigned</span> <span class="keyword">int</span> i = <span class="number">0</span>;; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">bool</span> completed = <span class="literal">false</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; NUM_OF_CAM; j++)</span><br><span class="line">            <span class="keyword">if</span> (j != <span class="number">1</span> || !STEREO_TRACK)</span><br><span class="line">                <span class="comment">////更新特征点id</span></span><br><span class="line">                completed |= trackerData[j].updateID(i);</span><br><span class="line">        <span class="keyword">if</span> (!completed)</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">    &#125;</span><br><span class="line"><span class="comment">//初始FeatureTracker::n_id = 0;id初始化-1</span></span><br><span class="line"><span class="keyword">bool</span> FeatureTracker::updateID(<span class="keyword">unsigned</span> <span class="keyword">int</span> i) &#123;</span><br><span class="line">    <span class="keyword">if</span> (i &lt; ids.size()) &#123;</span><br><span class="line">        <span class="keyword">if</span> (ids[i] == <span class="number">-1</span>)</span><br><span class="line">            ids[i] = n_id++;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">    &#125; <span class="keyword">else</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>7、如果PUB_THIS_FRAME=1则进行发布<blockquote>
<ul>
<li>将特征点id，矫正后归一化平面的3D点(x,y,z=1)，像素2D点(u,v)，像素的速度(vx,vy)，封装成sensor_msgs::PointCloudPtr类型的feature_points实例中,发布到pub_img;</li>
<li>将图像封装到cv_bridge::cvtColor类型的ptr实例中发布到pub_match</li>
</ul>
</blockquote>
</li>
</ul>
<h2 id="图像特征跟踪FeatureTracker-readImage"><a href="#图像特征跟踪FeatureTracker-readImage" class="headerlink" title="图像特征跟踪FeatureTracker::readImage()"></a>图像特征跟踪FeatureTracker::readImage()</h2><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">void</span> FeatureTracker::readImage(<span class="keyword">const</span> cv::Mat &amp;_img, <span class="keyword">double</span> _cur_time)</span><br></pre></td></tr></table></figure>
<ul>
<li>1、createCLAHE() 判断并对图像进行自适应直方图均衡化；</li>
<li>2、calcOpticalFlowPyrLK() 从cur_pts到forw_pts做LK金字塔光流法；</li>
<li>3、根据status，把跟踪失败的和位于图像边界外的点剔除，剔除时不仅要从当前帧数据forw_pts中剔除，而且还要从cur_un_pts、prev_pts、cur_pts，记录特征点id的ids，和记录特征点被跟踪次数的track_cnt中剔除；</li>
<li>4、setMask() 对跟踪点进行排序并依次选点，设置mask：去掉密集点，使特征点分布均匀</li>
<li>5、rejectWithF() 通过基本矩阵F剔除outliers</li>
<li>6、goodFeaturesToTrack() 寻找新的特征点(shi-tomasi角点)，添加(MAX_CNT - forw_pts.size())个点以确保每帧都有足够的特征点</li>
<li>7、addPoints()向forw_pts添加新的追踪点，id初始化-1，track_cnt初始化为1</li>
<li>8、undistortedPoints() 对特征点的图像坐标根据不同的相机模型进行去畸变矫正和深度归一化，计算每个角点的速度</li>
</ul>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">void</span> FeatureTracker::readImage(<span class="keyword">const</span> cv::Mat &amp;_img, <span class="keyword">double</span> _cur_time)</span><br><span class="line">&#123;</span><br><span class="line">    cv::Mat img;</span><br><span class="line">    TicToc t_r;</span><br><span class="line">    cur_time = _cur_time;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//如果EQUALIZE=1，表示太亮或则太暗</span></span><br><span class="line">    <span class="keyword">if</span> (EQUALIZE)<span class="comment">//判断是否进行直方图均衡化处理</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment">//自适应直方图均衡</span></span><br><span class="line">        <span class="comment">//createCLAHE(double clipLimit, Size tileGridSize)</span></span><br><span class="line">        cv::Ptr&lt;cv::CLAHE&gt; clahe = cv::createCLAHE(<span class="number">3.0</span>, cv::Size(<span class="number">8</span>, <span class="number">8</span>));</span><br><span class="line">        TicToc t_c;</span><br><span class="line">        clahe-&gt;apply(_img, img);</span><br><span class="line">        ROS_DEBUG(<span class="string">"CLAHE costs: %fms"</span>, t_c.toc());</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        img = _img;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (forw_img.empty())</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment">//如果当前帧的图像数据forw_img为空，说明当前是第一次读入图像数据</span></span><br><span class="line">        <span class="comment">//将读入的图像赋给当前帧forw_img</span></span><br><span class="line">        <span class="comment">//同时，还将读入的图像赋给prev_img、cur_img，这是为了避免后面使用到这些数据时，它们是空的</span></span><br><span class="line">        prev_img = cur_img = forw_img = img;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment">//否则，说明之前就已经有图像读入</span></span><br><span class="line">        <span class="comment">//所以只需要更新当前帧forw_img的数据</span></span><br><span class="line">        forw_img = img;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    forw_pts.clear();<span class="comment">//此时forw_pts还保存的是上一帧图像中的特征点，所以把它清除</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (cur_pts.size() &gt; <span class="number">0</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        TicToc t_o;</span><br><span class="line">        <span class="built_in">vector</span>&lt;uchar&gt; status;</span><br><span class="line">        <span class="built_in">vector</span>&lt;<span class="keyword">float</span>&gt; err;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//调用cv::calcOpticalFlowPyrLK()对前一帧的特征点cur_pts进行LK金字塔光流跟踪，得到forw_pts</span></span><br><span class="line">        <span class="comment">//status标记了从前一帧cur_img到forw_img特征点的跟踪状态，无法被追踪到的点标记为0</span></span><br><span class="line">        cv::calcOpticalFlowPyrLK(cur_img, forw_img, cur_pts, forw_pts, status, err, cv::Size(<span class="number">21</span>, <span class="number">21</span>), <span class="number">3</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//将位于图像边界外的点标记为0</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="keyword">int</span>(forw_pts.size()); i++)</span><br><span class="line">            <span class="keyword">if</span> (status[i] &amp;&amp; !inBorder(forw_pts[i]))</span><br><span class="line">                status[i] = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//根据status,把跟踪失败的点剔除</span></span><br><span class="line">        <span class="comment">//不仅要从当前帧数据forw_pts中剔除，而且还要从cur_un_pts、prev_pts和cur_pts中剔除</span></span><br><span class="line">        <span class="comment">//prev_pts和cur_pts中的特征点是一一对应的</span></span><br><span class="line">        <span class="comment">//记录特征点id的ids，和记录特征点被跟踪次数的track_cnt也要剔除</span></span><br><span class="line">        reduceVector(prev_pts, status);</span><br><span class="line">        reduceVector(cur_pts, status);</span><br><span class="line">        reduceVector(forw_pts, status);</span><br><span class="line">        reduceVector(ids, status);</span><br><span class="line">        reduceVector(cur_un_pts, status);</span><br><span class="line">        reduceVector(track_cnt, status);</span><br><span class="line">        ROS_DEBUG(<span class="string">"temporal optical flow costs: %fms"</span>, t_o.toc());</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//光流追踪成功,特征点被成功跟踪的次数就加1</span></span><br><span class="line">    <span class="comment">//数值代表被追踪的次数，数值越大，说明被追踪的就越久</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">auto</span> &amp;n : track_cnt)</span><br><span class="line">        n++;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//PUB_THIS_FRAME=1 需要发布特征点</span></span><br><span class="line">    <span class="keyword">if</span> (PUB_THIS_FRAME)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment">//通过基本矩阵剔除outliers</span></span><br><span class="line">        rejectWithF();</span><br><span class="line"></span><br><span class="line">        ROS_DEBUG(<span class="string">"set mask begins"</span>);</span><br><span class="line">        TicToc t_m;</span><br><span class="line"></span><br><span class="line">        setMask();<span class="comment">//保证相邻的特征点之间要相隔30个像素,设置mask</span></span><br><span class="line">        ROS_DEBUG(<span class="string">"set mask costs %fms"</span>, t_m.toc());</span><br><span class="line"></span><br><span class="line">        ROS_DEBUG(<span class="string">"detect feature begins"</span>);</span><br><span class="line">        TicToc <span class="keyword">t_t</span>;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//计算是否需要提取新的特征点</span></span><br><span class="line">        <span class="keyword">int</span> n_max_cnt = MAX_CNT - <span class="keyword">static_cast</span>&lt;<span class="keyword">int</span>&gt;(forw_pts.size());</span><br><span class="line">        <span class="keyword">if</span> (n_max_cnt &gt; <span class="number">0</span>)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">if</span>(mask.empty())</span><br><span class="line">                <span class="built_in">cout</span> &lt;&lt; <span class="string">"mask is empty "</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">            <span class="keyword">if</span> (mask.type() != CV_8UC1)</span><br><span class="line">                <span class="built_in">cout</span> &lt;&lt; <span class="string">"mask type wrong "</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">            <span class="keyword">if</span> (mask.size() != forw_img.size())</span><br><span class="line">                <span class="built_in">cout</span> &lt;&lt; <span class="string">"wrong size "</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">            <span class="comment">/** </span></span><br><span class="line"><span class="comment">            * cv::goodFeaturesToTrack()</span></span><br><span class="line"><span class="comment">            * @brief   在mask中不为0的区域检测新的特征点</span></span><br><span class="line"><span class="comment">            * @optional    ref:https://docs.opencv.org/3.1.0/dd/d1a/group__imgproc__feature.html#ga1d6bb77486c8f92d79c8793ad995d541</span></span><br><span class="line"><span class="comment">            * @param[in]    InputArray _image=forw_img 输入图像</span></span><br><span class="line"><span class="comment">            * @param[out]   _corners=n_pts 存放检测到的角点的vector</span></span><br><span class="line"><span class="comment">            * @param[in]    maxCorners=MAX_CNT - forw_pts.size() 返回的角点的数量的最大值</span></span><br><span class="line"><span class="comment">            * @param[in]    qualityLevel=0.01 角点质量水平的最低阈值（范围为0到1，质量最高角点的水平为1），小于该阈值的角点被拒绝</span></span><br><span class="line"><span class="comment">            * @param[in]    minDistance=MIN_DIST 返回角点之间欧式距离的最小值</span></span><br><span class="line"><span class="comment">            * @param[in]    _mask=mask 和输入图像具有相同大小，类型必须为CV_8UC1,用来描述图像中感兴趣的区域，只在感兴趣区域中检测角点</span></span><br><span class="line"><span class="comment">            * @param[in]    blockSize：计算协方差矩阵时的窗口大小</span></span><br><span class="line"><span class="comment">            * @param[in]    useHarrisDetector：指示是否使用Harris角点检测，如不指定，则计算shi-tomasi角点</span></span><br><span class="line"><span class="comment">            * @param[in]    harrisK：Harris角点检测需要的k值</span></span><br><span class="line"><span class="comment">            * @return      void</span></span><br><span class="line"><span class="comment">            */</span></span><br><span class="line">            cv::goodFeaturesToTrack(forw_img, n_pts, MAX_CNT - forw_pts.size(), <span class="number">0.01</span>, MIN_DIST, mask);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            n_pts.clear();</span><br><span class="line">        ROS_DEBUG(<span class="string">"detect feature costs: %fms"</span>, <span class="keyword">t_t</span>.toc());</span><br><span class="line"></span><br><span class="line">        ROS_DEBUG(<span class="string">"add feature begins"</span>);</span><br><span class="line">        TicToc t_a;</span><br><span class="line"></span><br><span class="line">        <span class="comment">//添将新检测到的特征点n_pts添加到forw_pts中，id初始化-1,track_cnt初始化为1.</span></span><br><span class="line">        addPoints();</span><br><span class="line"></span><br><span class="line">        ROS_DEBUG(<span class="string">"selectFeature costs: %fms"</span>, t_a.toc());</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//当下一帧图像到来时，当前帧数据就成为了上一帧发布的数据</span></span><br><span class="line">    prev_img = cur_img;</span><br><span class="line">    prev_pts = cur_pts;</span><br><span class="line">    prev_un_pts = cur_un_pts;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//把当前帧的数据forw_img、forw_pts赋给上一帧cur_img、cur_pts</span></span><br><span class="line">    cur_img = forw_img;</span><br><span class="line">    cur_pts = forw_pts;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//根据不同的相机模型进行去畸变矫正和深度归一化，计算速度</span></span><br><span class="line">    undistortedPoints();</span><br><span class="line">    prev_time = cur_time;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>待写！！！</p>
<p>参考：<a href="https://blog.csdn.net/qq_41839222/article/details/85797156" target="_blank" rel="noopener">VINS-Mono代码解读——视觉跟踪 feature_trackers</a><br><a href="https://blog.csdn.net/wangshuailpp/article/details/78719401" target="_blank" rel="noopener">VINS理论与代码详解2——单目视觉跟踪</a><br><a href="https://blog.csdn.net/q597967420/article/details/76099425" target="_blank" rel="noopener">VINS-Mono源码解析（二）前端：特征跟踪</a></p>
]]></content>
      <categories>
        <category>VINS</category>
      </categories>
      <tags>
        <tag>VINS</tag>
      </tags>
  </entry>
  <entry>
    <title>IMU测量模型、运动模型、误差模型</title>
    <url>/2019/10/22/IMU%E6%B5%8B%E9%87%8F%E6%A8%A1%E5%9E%8B%E3%80%81%E8%BF%90%E5%8A%A8%E6%A8%A1%E5%9E%8B%E3%80%81%E8%AF%AF%E5%B7%AE%E6%A8%A1%E5%9E%8B/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="IMU测量模型"><a href="#IMU测量模型" class="headerlink" title="IMU测量模型"></a>IMU测量模型</h1><ul>
<li><p>MEMS 加速度计工作原理<br>测量原理可以用一个简单的质量块 + 弹簧 + 指示计来表示<br>加速度计测量值$a_m$为弹簧拉力对应的加速度</p>
<script type="math/tex; mode=display">a_m=\frac{f}{m}=a-g</script><p>其中m为质量块质量，a为物体在世界（惯性）坐标系下的实际加速度。</p>
</li>
<li><p>陀螺仪测量原理<br>陀螺仪的测量值即为IMU在body坐标系下的旋转角速度，要通过运动学模型转换到惯性坐标系下 ，按测量原理分有振动陀螺，光纤陀螺等。</p>
<p>低端 MEMS 陀螺上一般采用振动陀螺原理，通过测量 Coriolisforce 来间接得到角速度。</p>
<blockquote>
<p>在旋转坐标系中，运动的物体受到科氏力作用<br>MEMS 陀螺仪：一个主动运动轴 + 一个敏感轴<br><img alt="在这里插入图片描述" data-src="https://img-blog.csdnimg.cn/20191017192448388.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjkwNTE0MQ==,size_16,color_FFFFFF,t_70#pic_center"></p>
<h1 id="IMU运动模型"><a href="#IMU运动模型" class="headerlink" title="IMU运动模型"></a>IMU运动模型</h1><h2 id="旋转量求导"><a href="#旋转量求导" class="headerlink" title="旋转量求导"></a>旋转量求导</h2><p>首先，如下图所示，考虑一个从原点出发的向量 r 绕单位轴 u 旋转，角速度大小为 θ˙。<br><img alt="在这里插入图片描述" data-src="https://img-blog.csdnimg.cn/20191017155511191.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjkwNTE0MQ==,size_16,color_FFFFFF,t_70"><br>角速度矢量可以表示为${\boldsymbol \omega}=\dot{\theta}u$，易得向量 r 末端点 P 的速度矢量，即 r的时间一阶导为</p>
<script type="math/tex; mode=display">\frac{d{r}}{dt} = {\boldsymbol \omega} \times {r}</script><p>坐标系 {B} 绕单位轴 u 旋转，如上所述，其三个轴的时间一阶导同样为</p>
<script type="math/tex; mode=display">\frac{d{\bf i}_B}{dt} = {\boldsymbol \omega} \times {\bf i}_B, \frac{d{\bf j}_B}{dt} = {\boldsymbol \omega} \times {\bf j}_B, \frac{d{\bf k}_B}{dt} = {\boldsymbol \omega} \times {\bf k}_B</script><p>$[ {\bf i}_B \quad {\bf j}_B \quad {\bf k}_B ]$ 实际上就是坐标系 {B} 相对于参考坐标系的旋转矩阵 R，R的时间一阶导为</p>
<script type="math/tex; mode=display">\dot{\bf R} =  [ {\boldsymbol \omega} \times {\bf i}_B \quad {\boldsymbol \omega} \times {\bf j}_B \quad {\boldsymbol \omega} \times {\bf k}_B ] = {\boldsymbol \omega} \times {\bf R}</script><p>叉乘运算可以转化为负对称矩阵的乘法：</p>
<script type="math/tex; mode=display">\dot{\bf R} = {\boldsymbol \omega}^{\land} {\bf R}</script><p>其中负对称矩阵${\boldsymbol \omega}^{\land}$为</p>
<script type="math/tex; mode=display">% <![CDATA[
\quad {\boldsymbol \omega}^{\land}= \begin{bmatrix}0 & -\omega_3 & \omega_2\\ \omega_3 & 0 & -\omega_1 \\ -\omega_2 & \omega_1 & 0\end{bmatrix} %]]></script><p>这里的角速度 ω 是在参考坐标系下表达的，角速度也经常表达在体坐标系 {B} 下，记为${}^B{\boldsymbol \omega} = {\bf R}^T{\boldsymbol \omega}$，即${\boldsymbol \omega} = {\bf R}{}^B{\boldsymbol \omega}$<br>上式也可以写作：</p>
<script type="math/tex; mode=display">\dot{\bf R} =w^{\land}R=(R^Bw)^{\land}R=R{}^{\land}wR^TR= R({}^Bw)^{\land}</script></blockquote>
</li>
</ul><a id="more"></a>
<blockquote>
<p>证：对任意旋转矩阵 R 和三维向量 v，都有$({\bf R v})^{\land} = {\bf Rv^{\land}R}^T$<br><img alt="在这里插入图片描述" data-src="https://img-blog.csdnimg.cn/20191017160718789.png#pic_center"><br>最后一式利用向量叉乘的旋转变换不变性可证，即，对于任意${\bf v,u}\in \mathbb{R}^3$永远有</p>
<script type="math/tex; mode=display">({\bf Rv})\times({\bf Ru})={\bf R(v\times u)}</script><p>从三维几何的角度来理解：v,u 是任意两个三维向量，(v×u) 是一个和 v,u 都垂直、大小为 |v||u|sin(u,v) 的三维向量；将 v,u,v×u 三个向量都经过同一个旋转，它们的相对位姿和模长都不会改变，所以 (Rv) 和 (Ru) 的叉乘仍是 R(v×u)。</p>
</blockquote>
<h2 id="科氏加速度"><a href="#科氏加速度" class="headerlink" title="科氏加速度"></a>科氏加速度</h2><p>这次把绕惯性系 {A} 中固定单位轴 u 旋转的 {B} 作为参考坐标系。考虑下图， 点 P 相对于 {B} 运动，记 ${}^Br$ 分别为 P 在 {B} 下的坐标，r 为 P 的绝对坐标（即 {A} 下坐标）， R 仍为 {B} 相对于 {A} 的旋转矩阵，易知 $r=R{}^Br$。<br><img alt="在这里插入图片描述" data-src="https://img-blog.csdnimg.cn/20191017163839187.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjkwNTE0MQ==,size_16,color_FFFFFF,t_70#pic_center"><br>$r=R{}^Br$求一阶时间导</p>
<script type="math/tex; mode=display">{\bf v} = \dot{\bf r} = \dot{\bf R} {}^B{\bf r} + {\bf R}^B\dot{\bf r} = {\boldsymbol \omega}^{\land}{\bf R}{}^B{\bf r}+ {\bf R}^B\dot{\bf r}</script><p>记P在 {B}下速度为 ${}^Bv$，于是</p>
<script type="math/tex; mode=display">v = w^{\land}r+ {R}^B{}= w \times r+ v_r</script><p>${\bf v}_r$来表达「相对速度」的概念，准确定义为 P 相对于 {B} 的速度，在惯性系 {A} 下的表达<br>再对求时间导：</p>
<blockquote>
<p><img alt="在这里插入图片描述" data-src="https://img-blog.csdnimg.cn/20191017165037235.png#pic_center"><br>在旋转坐标系下观察，运动的物体（运动方向和旋转轴不为同一个轴时）会受到科氏力的作用。</p>
<ul>
<li>第一项中${\boldsymbol \alpha}$为 {B} 的角加速度，所以第一项的物理意义是 {B} 旋转所造成的 P 的切向加速度</li>
<li>第二项是 {B} 旋转所造成的向心加速度</li>
<li>第三项比较特殊，为 {B} 的旋转运动与 P 相对 {B} 的平移运动耦合产生的加速度，称为科氏加速度</li>
<li>第四项为 P 相对于 {B} 的加速度，但在惯性系{A}下表达，类似于${\bf v}_r$</li>
</ul>
</blockquote>
<h1 id="IMU-误差模型"><a href="#IMU-误差模型" class="headerlink" title="IMU 误差模型"></a>IMU 误差模型</h1><p>加速度计和陀螺仪的误差可以分为：确定性误差，随机误差。</p>
<ul>
<li>确定性误差可以事先标定确定，包括： bias, scale …</li>
<li>随机误差通常假设噪声服从高斯分布，包括：高斯白噪声， bias<br>随机游走…<h2 id="确定性误差"><a href="#确定性误差" class="headerlink" title="确定性误差"></a>确定性误差</h2></li>
</ul>
<ol>
<li>Bias ，Scale<br>理论上，当没有外部作用时， IMU 传感器的输出应该为 0，但实际数据存在一个偏置 b。</li>
</ol>
<ul>
<li>加速度计 bias 对位姿估计的影响：<script type="math/tex; mode=display">v_err=b_at，p_err=\frac{1}{2}b_at^2</script></li>
<li>scale 可以看成是实际数值和传感器输出值之间的比值</li>
</ul>
<ol>
<li>Nonorthogonality/Misalignment Errors(非正交/错位错误）<br><img alt data-src="https://img-blog.csdnimg.cn/20191017182956354.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjkwNTE0MQ==,size_16,color_FFFFFF,t_70"> </li>
<li>其他确定性误差<blockquote>
<ul>
<li>Run-to-Run Bias/Scale Facto</li>
<li>In Run (Stability) Bias/Scale Factor</li>
<li>Temperature-Dependent Bias/Scale Factor</li>
</ul>
</blockquote>
</li>
</ol>
<h3 id="确定性误差误差标定"><a href="#确定性误差误差标定" class="headerlink" title="确定性误差误差标定"></a>确定性误差误差标定</h3><h4 id="六面法标定加速度"><a href="#六面法标定加速度" class="headerlink" title="六面法标定加速度"></a>六面法标定加速度</h4><p><strong>bias 和 scale factor</strong></p>
<blockquote>
<p>六面法是指将加速度计的 3 个轴分别朝上或者朝下水平放置一段时间，采集 6 个面的数据完成标定。如果各个轴都是正交的，那很容易得到 bias 和 scale：<br><img alt="在这里插入图片描述" data-src="https://img-blog.csdnimg.cn/20191017171617729.png#pic_center"><br>其中,$l$为加速度计某个轴的测量值， g为当地的重力加速度</p>
</blockquote>
<p><strong>（Nonorthogonality/Misalignment Errors)</strong></p>
<blockquote>
<p>多轴 IMU 传感器制作的时候，由于制作工艺的问题，会使得 xyz 轴可能不垂直，如下图所示。</p>
<p>考虑轴间误差的时候，实际加速度和测量值之间的关系为：<br><img alt="在这里插入图片描述" data-src="https://img-blog.csdnimg.cn/20191017172240109.png#pic_center"><br>水平静止放置 6 面的时候，加速度的理论值为<br><img alt="在这里插入图片描述" data-src="https://img-blog.csdnimg.cn/20191017172303634.png#pic_center"><br>对应的测量值矩阵 L ：<br><img alt="在这里插入图片描述" data-src="https://img-blog.csdnimg.cn/20191017172326866.png#pic_center"><br>利用最小二乘就能够把12个变量求出来。</p>
</blockquote>
<h4 id="六面法标定陀螺仪"><a href="#六面法标定陀螺仪" class="headerlink" title="六面法标定陀螺仪"></a>六面法标定陀螺仪</h4><blockquote>
<ul>
<li>六面法标定陀螺仪 bias 和 scale factor<br>和加速度计六面法不同的是，陀螺仪的真实值由高精度转台提供，这<br>里的 6 面是指各个轴顺时针和逆时针旋转。<br><img alt="在这里插入图片描述" data-src="https://img-blog.csdnimg.cn/20191017182109980.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjkwNTE0MQ==,size_16,color_FFFFFF,t_70#pic_center"><br>参考：<a href="https://blog.csdn.net/CSDN_XCS/article/details/90339761" target="_blank" rel="noopener">https://blog.csdn.net/CSDN_XCS/article/details/90339761</a></li>
</ul>
</blockquote>
<h4 id="温度相关的参数标定"><a href="#温度相关的参数标定" class="headerlink" title="温度相关的参数标定"></a>温度相关的参数标定</h4><ul>
<li>目的：这个标定的主要目的是对传感器估计的 bias 和 scale 进行温度补偿，获取不同温度时 bias 和 scale 的值，绘制成曲线。</li>
<li>两种标定方法：<blockquote>
<p>• soak method: 控制恒温室的温度值，然后读取传感器数值进行标定。<br>• ramp method：记录一段时间内线性升温和降温时传感器的数据来进行标定。</p>
</blockquote>
</li>
</ul>
<h2 id="随机误差"><a href="#随机误差" class="headerlink" title="随机误差"></a>随机误差</h2><h3 id="高斯白噪声与随机游走"><a href="#高斯白噪声与随机游走" class="headerlink" title="高斯白噪声与随机游走"></a>高斯白噪声与随机游走</h3><h4 id="高斯白噪声"><a href="#高斯白噪声" class="headerlink" title="高斯白噪声"></a>高斯白噪声</h4><p>高斯白噪声的高斯指的是概率分布为正态分布，白噪声指的是其二阶矩不相关，一阶矩为常数。故把瞬时值的概率分布服从高斯分布，功率谱密度服从均匀分布的噪声称为高斯白噪声</p>
<p>IMU 数据连续时间上受到一个均值为 0，方差为 $σ^2$，各时刻之间相互独立的高斯过程 n(t)：</p>
<script type="math/tex; mode=display">E[n(t)]=0</script><script type="math/tex; mode=display">E[n(t_1​)n(t_2​)]=σ^2δ(t_1​−t_2​)</script><p>自相关函数的数学定义：$R(τ)=∫^{−∞}_∞　x(t)x(t−τ)dt$</p>
<p>其中 $δ()$ 表示狄拉克函数<br><img alt="在这里插入图片描述" data-src="https://img-blog.csdnimg.cn/20190929113450104.png#pic_center"><br><strong>白噪声的离散化</strong><br>实际上， IMU 传感器获取的数据为离散采样，离散和连续高斯白噪声的方差之间存在如下转换关系：</p>
<script type="math/tex; mode=display">n_d[k] \triangleq n(t_0+\Delta t)\simeq\frac{1}{\Delta t}\int_{t_0}^{t_0+\Delta t}n(\tau)dt</script><script type="math/tex; mode=display">
 \begin{aligned}
E(n_d[k]^2) & = E(\frac{1}{\Delta t^2}\int_{t_0}^{t_0+\Delta t}\int_{t_0}^{t_0+\Delta t}n(\tau)n(t)d \tau dt) \\
&= E( \frac{\sigma^2}{\Delta t^2}\int_{t_0}^{t_0+\Delta t}\int_{t_0}^{t_0+\Delta t}\delta(t-\tau)d \tau dt)\\
&= E(\frac{\sigma^2}{\Delta t})
\end{aligned}</script><p>即:</p>
<script type="math/tex; mode=display">nd[k] = σ_dw[k]</script><p>其中：$w[k] ∼ N(0, 1)$，$\sigma_d=\sigma \frac{1}{\sqrt{\bigtriangleup t}}$<br>也就是说高斯白噪声的连续时间到离散时间之间差一个 $\frac{1}{\sqrt{\bigtriangleup t}}$ ，√∆t 是传感器的采样时间。</p>
<h4 id="Bias-随机游走"><a href="#Bias-随机游走" class="headerlink" title="Bias 随机游走"></a>Bias 随机游走</h4><p>通常用维纳过程 (wiener process) 来建模 bias 随时间连续变化的过程，离散时间下称之为随机游走</p>
<script type="math/tex; mode=display">\dot{b}_{(t)}=n(t)=\sigma_bw(t)</script><p>其中 w 是方差为 1 的白噪声</p>
<p>离散和连续之间的转换：</p>
<script type="math/tex; mode=display">b_d[k] \triangleq b(t_0) + \int_{t_0}^{t_0+\Delta t}n(t)dt$$$$\begin{aligned}
E((b_d[k]-b_d[k-1])^2) 
&=E(\int_{t_0}^{t_0+\Delta t}\int_{t_0}^{t_0+\Delta t}n(t)n(\tau)d \tau dt)\\
&= E({\sigma_{b}^2}\int_{t_0}^{t_0+\Delta t}\int_{t_0}^{t_0+\Delta t}\delta(t-\tau)d \tau dt)\\
&= E(\sigma_{b}^2\Delta t)
\end{aligned}</script><p>即：</p>
<script type="math/tex; mode=display">b_d[k] = b_d[k − 1] + σ_{bd}w[k]</script><p>其中：</p>
<script type="math/tex; mode=display">w[k] ∼ N(0, 1),σ_{bd} = σ_b\sqrt{\bigtriangleup t}</script><p>bias 随机游走的噪声方差从连续时间到离散之间需要乘以 $\sqrt{\bigtriangleup t}$ </p>
<h3 id="随机误差的标定"><a href="#随机误差的标定" class="headerlink" title="随机误差的标定"></a>随机误差的标定</h3><h4 id="艾伦方差标定"><a href="#艾伦方差标定" class="headerlink" title="艾伦方差标定"></a>艾伦方差标定</h4><p>Allan 方差法是 20 世纪 60 年代由美国国家标准局的 David Allan 提出的，它是一种基于时域的分析方法。</p>
<blockquote>
<p>具体的流程如下：</p>
<ol>
<li>保持传感器绝对静止获取数据</li>
<li>对数据进行分段，设定时间段的时长，如下图所示<br><img alt="在这里插入图片描述" data-src="https://img-blog.csdnimg.cn/20191017204504508.png#pic_center"></li>
<li>将传感器数据按照时间段进行平均</li>
<li>计算方差，绘制艾伦曲线，得到的艾伦曲线如下图所示:<br><img alt="在这里插入图片描述" data-src="https://img-blog.csdnimg.cn/20191017204557636.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjkwNTE0MQ==,size_16,color_FFFFFF,t_70">Allan方差法可用于5种随机误差的标定：<br><img alt="在这里插入图片描述" data-src="https://img-blog.csdnimg.cn/20191017212131103.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjkwNTE0MQ==,size_16,color_FFFFFF,t_70"><br>参考：<a href="https://blog.csdn.net/YunLaowang/article/details/95608107" target="_blank" rel="noopener">VIO标定IMU随机误差：Allan方差法</a><br><a href="https://blog.csdn.net/lei1105034103/article/details/89159459" target="_blank" rel="noopener">陀螺 Allan 方差分析</a></li>
</ol>
</blockquote>
<h2 id="IMU-数学模型"><a href="#IMU-数学模型" class="headerlink" title="IMU 数学模型"></a>IMU 数学模型</h2><p>以 ECI 为参考坐标系（由于 ECI 为惯性系，不需要考虑地球自转）：<br>忽略 scale 的影响，只考虑白噪声和 bias 随机游走：<br>IMU的真实值：$w^b,a^b$<br>IMU的测量值：$\widetilde{w},\widetilde{a}^b$<br>IMU机体系即b系，w系表示世界坐标系。</p>
<script type="math/tex; mode=display">\begin{aligned}
 \widetilde{w}&=w^b +b^g+n^g\\
 \widetilde{a}^b&=a^b+b^a+n^a\\&=q_{bw}(a^w+g^w)+b^a+n^a
 \end{aligned}</script><p>考虑高斯白噪声项，有：</p>
<script type="math/tex; mode=display">\begin{aligned}
 &w^b=\widetilde{w} -b^g-n^g\\
 &a^w=q_{wb}(\widetilde{a}^b-b^a-n^a)-g^w
 \end{aligned}</script><p>不考虑高斯白噪声项，有：</p>
<script type="math/tex; mode=display">\begin{aligned}
 &w^b=\widetilde{w} -b^g\\
 &a^w=q_{wb}(\widetilde{a}^b-b^a)-g^w
 \end{aligned}</script><p>参考：<a href="https://fzheng.me/2016/11/20/imu_model_eq/#1-2-%E5%89%8D%E7%BD%AE2-%E5%9B%9B%E5%85%83%E6%95%B0" target="_blank" rel="noopener">从零开始的 IMU 状态模型推导</a></p>
]]></content>
      <categories>
        <category>VIO</category>
      </categories>
  </entry>
  <entry>
    <title>随机抽样一致RANSAC-Random-Sample-Consensus</title>
    <url>/2019/10/22/%E9%9A%8F%E6%9C%BA%E6%8A%BD%E6%A0%B7%E4%B8%80%E8%87%B4RANSAC-Random-Sample-Consensus/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="算法简介："><a href="#算法简介：" class="headerlink" title="算法简介："></a>算法简介：</h1><p>随机抽样一致算法（RANdom SAmple Consensus，RANSAC）。它采用迭代的方式从一组包含离群的被观测数据中估算出数学模型的参数。RANSAC算法的基本假设是样本中包含正确数据(inliers，可以被模型描述的数据)，也包含异常数据(outliers，偏离正常范围很远、无法适应数学模型的数据)，即数据集中含有噪声。这些异常数据可能是由于错误的测量、错误的假设、错误的计算等产生的。同时RANSAC也假设，给定一组正确的数据，存在可以计算出符合这些数据的模型参数的方法。</p><a id="more"></a>
<h1 id="RANSAC的基本假设："><a href="#RANSAC的基本假设：" class="headerlink" title="RANSAC的基本假设："></a>RANSAC的基本假设：</h1><p>“内群”数据可以通过几组模型的参数来叙述其分布，而“离群”数据则是不适合模型化的数据。<br>数据会受噪声影响，噪声指的是离群，例如从极端的噪声或错误解释有关数据的测量或不正确的假设。<br>RANSAC假定，给定一组（通常很小）的内群，存在一个程序，这个程序可以估算最佳解释或最适用于这一数据模型的参数。</p>
<h1 id="范例"><a href="#范例" class="headerlink" title="范例"></a>范例</h1><p>这里用一个简单的例子来说明，在一组数据点中找到一条最适合的线。假设，此有一组集合包含了内群以及离群，其中内群为可以被拟合到线段上的点，而离群则是无法被拟合的点。如果我们用简单的最小二乘法来找此线，我们将无法得到一条适合于内群的线，因为最小二乘法会受离群影响而影响其结果。而RANSAC，可以只由内群来计算出模型，而且概率还够高。然而，RANSAC无法保证结果一定最好，所以必须小心选择参数，使其能有足够的概率。<br><img alt="在这里插入图片描述" data-src="https://img-blog.csdnimg.cn/20191020160728989.png#pic_center"><br>包含许多离群的一组数据，要找一条最适合的线。<br><img alt="在这里插入图片描述" data-src="https://img-blog.csdnimg.cn/201910201607364.png#pic_center"><br>RANSAC找到的线，离群值对结果没影响（蓝色点为内群，红色点为离群）</p>
<h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h1><p>RANSAC算法的输入是一组观测数据（往往含有较大的噪声或无效点），一个用于解释观测数据的参数化模型以及一些可信的参数。RANSAC通过反复选择数据中的一组随机子集来达成目标。被选取的子集被假设为局内点，并用下述方法进行验证： </p>
<ol>
<li>有一个模型适应于假设的局内点，即所有的未知参数都能从假设的局内点计算得出。</li>
<li>用1中得到的模型去测试所有的其它数据，如果某个点适用于估计的模型，认为它也是局内点。</li>
<li>如果有足够多的点被归类为假设的局内点，那么估计的模型就足够合理。</li>
<li>然后，用所有假设的局内点去重新估计模型（譬如使用最小二乘法），因为它仅仅被初始的假设局内点估计过。</li>
<li>最后，通过估计局内点与模型的错误率来评估模型。</li>
<li>上述过程被重复执行固定的次数，每次产生的模型要么因为局内点太少而被舍弃，要么因为比现有的模型更好而被选用。<h1 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h1>伪码形式的算法如下所示：</li>
</ol>
<ul>
<li><p>输入：</p>
<blockquote>
<p>data —— 一组观测数据<br>model —— 适应于数据的模型<br>n —— 适用于模型的最少数据个数<br>k —— 算法的迭代次数<br>t —— 用于决定数据是否适应于模型的阀值<br>d —— 判定模型是否适用于数据集的数据数目</p>
</blockquote>
</li>
<li><p>输出：</p>
<blockquote>
<p>best_model —— 跟数据最匹配的模型参数（如果没有找到好的模型，返回null）<br>best_consensus_set —— 估计出模型的数据点<br>best_error —— 跟数据相关的估计出的模型错误</p>
</blockquote>
<ul>
<li>iterations = 0<blockquote>
<p>best_model = null<br>best_consensus_set = null<br>best_error = 无穷大<br>while ( iterations &lt; k )<br>maybe_inliers = 从数据集中随机选择n个点<br>maybe_model = 适合于maybe_inliers的模型参数<br>consensus_set = maybe_inliers</p>
</blockquote>
</li>
</ul>
</li>
<li><p>估计的模型就足够合理</p>
<blockquote>
<p>for ( 每个数据集中不属于maybe_inliers的点 ）<br>if ( 如果点适合于maybe_model，且错误小于t ）<br>将点添加到consensus_set<br>if （ consensus_set中的元素数目大于d ）<br>已经找到了好的模型，</p>
</blockquote>
</li>
<li><p>现在测试该模型到底有多好</p>
<blockquote>
<p>better_model = 适合于consensus_set中所有点的模型参数<br>this_error = better_model究竟如何适合这些点的度量<br>if ( this_error &lt; best_error )<br>我们发现了比以前好的模型，保存该模型直到更好的模型出现<br>best_model =  better_model<br>best_consensus_set = consensus_set<br>best_error =  this_error</p>
</blockquote>
</li>
<li><p>迭代</p>
<blockquote>
<p>增加迭代次数<br>返回 best_model, best_consensus_set, best_error</p>
</blockquote>
<p> RANSAC算法的可能变化包括以下几种：</p>
<ol>
<li><p>如果发现了一种足够好的模型（该模型有足够小的错误率），则跳出主循环。这样可能会节约计算额外参数的时间。</p>
<ol>
<li>直接从maybe_model计算this_error，而不从consensus_set重新估计模型。这样可能会节约比较两种模型错误的时间，但可能会对噪声更敏感。</li>
</ol>
<p>其实核心就是随机性和假设性。随机性用于减少计算了，那个循环次数就是利用正确数据出现的概率。所谓的假设性，就是说随机抽出来的数据我都认为是正确的，并以此去计算其他点，获得其他满足变换关系的点，然后利用投票机制，选出获票最多的那一个变换。</p>
</li>
</ol>
</li>
</ul>
<h1 id="参数决定"><a href="#参数决定" class="headerlink" title="参数决定"></a>参数决定</h1><p>假设每个点是真正内群的几率是w，则：</p>
<ul>
<li>w = 真正内群的数目 / 数据总共的数量</li>
</ul>
<p>通常我们不知道w 是多少，$w^{n}$是所选择的n 个点都是内群的几率，$1-w^{n}$ 是所选择的 n 个点至少有一个不是内群的几率， ${(1-w^{n})^{k}}$是表示重复k 次都没有全部的 {\displaystyle n} n 个点都是内群的几率，假设算法跑 k 次以后成功的几率是p，那么：</p>
<script type="math/tex; mode=display">{1-p=(1-w^{n})^{k}}</script><script type="math/tex; mode=display">{p=1-(1-w^{n})^{k}}</script><p>所以如果希望成功几率高， ${\displaystyle p=0.99}$， 当n 不变时，k 越大， {\displaystyle p} p 越大， 当w 不变时，n 越大，所需的k 就越大， 通常w 未知，所以 n 选小一点比较好。</p>
<h1 id="优点与缺点"><a href="#优点与缺点" class="headerlink" title="优点与缺点"></a>优点与缺点</h1><ul>
<li>RANSAC的优点是它能鲁棒的估计模型参数。例如，它能从包含大量局外点的数据集中估计出高精度的参数。</li>
<li>RANSAC的缺点是它计算参数的迭代次数没有上限；如果设置迭代次数的上限，得到的结果可能不是最优的结果，甚至可能得到错误的结果。</li>
<li>RANSAC只有一定的概率得到可信的模型，概率与迭代次数成正比。RANSAC的另一个缺点是它要求设置跟问题相关的阀值。</li>
<li>RANSAC只能从特定的数据集中估计出一个模型，如果存在两个（或多个）模型，RANSAC不能找到别的模型。</li>
</ul>
<p>参考：</p>
<blockquote>
<p><a href="https://blog.csdn.net/pi9nc/article/details/26596519" target="_blank" rel="noopener">RANSAC</a><br><a href="https://zh.wikipedia.org/wiki/%E9%9A%A8%E6%A9%9F%E6%8A%BD%E6%A8%A3%E4%B8%80%E8%87%B4" target="_blank" rel="noopener">随机抽样一致</a></p>
</blockquote>
]]></content>
  </entry>
  <entry>
    <title>Hello World</title>
    <url>/2019/10/22/hello-world/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><a id="more"></a>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>
]]></content>
  </entry>
  <entry>
    <title>KLT 光流算法</title>
    <url>/2019/10/20/KLT-%E5%85%89%E6%B5%81%E7%AE%97%E6%B3%95/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="Optical-Flow"><a href="#Optical-Flow" class="headerlink" title="Optical Flow"></a>Optical Flow</h1><p><img alt="在这里插入图片描述" data-src="https://img-blog.csdnimg.cn/20191020132123717.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjkwNTE0MQ==,size_16,color_FFFFFF,t_70"><br><img alt="在这里插入图片描述" data-src="https://img-blog.csdnimg.cn/20191020132137931.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjkwNTE0MQ==,size_16,color_FFFFFF,t_70"></p><h1 id="KLT"><a href="#KLT" class="headerlink" title="KLT"></a>KLT</h1><p>KLT 算法本质上也基于光流的三个假设，不同于前述直接比较像素点灰度值的作法，KLT 比较像素点周围的窗口像素，来寻找最相似的像素点。<br><img alt="在这里插入图片描述" data-src="https://img-blog.csdnimg.cn/2019102013215737.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjkwNTE0MQ==,size_16,color_FFFFFF,t_70"></p><p><img alt="在这里插入图片描述" data-src="https://img-blog.csdnimg.cn/20191020132228980.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjkwNTE0MQ==,size_16,color_FFFFFF,t_70"><br>参考：<a href="https://blog.csdn.net/sgfmby1994/article/details/68489944" target="_blank" rel="noopener">总结：光流—LK光流—基于金字塔分层的LK光流—中值流</a><br><a href="https://leijiezhang001.github.io/KLT/" target="_blank" rel="noopener">KLT 光流算法详解</a><br><a href="https://blog.csdn.net/irobot_davinci/article/details/29635199" target="_blank" rel="noopener">KLT角点跟踪算法(LK)学习（一）——算法原理</a></p>]]></content>
      <categories>
        <category>VINS</category>
      </categories>
  </entry>
  <entry>
    <title>Harris角点检测</title>
    <url>/2019/10/20/Harris%E8%A7%92%E7%82%B9%E6%A3%80%E6%B5%8B/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><h1 id="何为角点"><a href="#何为角点" class="headerlink" title="何为角点"></a>何为角点</h1><p>下面有两幅不同视角的图像，通过找出对应的角点进行匹配。<br><img alt="在这里插入图片描述" data-src="https://img-blog.csdnimg.cn/20191020095257386.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjkwNTE0MQ==,size_16,color_FFFFFF,t_70#pic_center"><br>再看下图所示，放大图像的两处角点区域：<br><img alt="在这里插入图片描述" data-src="https://img-blog.csdnimg.cn/20191020095313814.png#pic_center"><br>角点在保留图像图形重要特征的同时，可以有效地减少信息的数据量，使其信息的含量很高，有效地提高了计算的速度，有利于图像的可靠匹配，使得实时处理成为可能。</p><p>我们可以直观的概括下角点所具有的特征：</p><ul>
<li>轮廓之间的交点；</li>
<li>对于同一场景，即使视角发生变化，通常具备稳定性质的特征；<ul>
<li>该点附近区域的像素点无论在梯度方向上还是其梯度幅值上有着较大变化；<h1 id="角点检测算法基本思想"><a href="#角点检测算法基本思想" class="headerlink" title="角点检测算法基本思想"></a>角点检测算法基本思想</h1>算法基本思想是使用一个固定窗口在图像上进行任意方向上的滑动，比较滑动前与滑动后两种情况，窗口中的像素灰度变化程度，如果存在任意方向上的滑动，都有着较大灰度变化，那么我们可以认为该窗口中存在角点。<h1 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h1></li>
</ul>
</li>
</ul><a id="more"></a>


<p>当窗口发生[u,v]移动时，那么滑动前与滑动后对应的窗口中的像素点灰度变化描述如下：</p>
<script type="math/tex; mode=display">E(u, v)=\sum_{x_{s}, y} w(x, y)[I(x+u, y+v)-I(x, y)]^{2}</script><p>[u,v]是窗口的偏移量<br>(x,y)是窗口内所对应的像素坐标位置，窗口有多大，就有多少个位置<br>w(x,y)是窗口函数，最简单情形就是窗口内的所有像素所对应的w权重系数均为1。但有时候，我们会将w(x,y)函数设定为以窗口中心为原点的二元正态分布。如果窗口中心点是角点时，移动前与移动后，该点的灰度变化应该最为剧烈，所以该点权重系数可以设定大些，表示窗口移动时，该点在灰度变化贡献较大；而离窗口中心(角点)较远的点，这些点的灰度变化几近平缓，这些点的权重系数，可以设定小点，以示该点对灰度变化贡献较小，那么我们自然想到使用二元高斯函数来表示窗口函数，所以通常窗口函数有如下两种形式：<br><img alt="在这里插入图片描述" data-src="https://img-blog.csdnimg.cn/20191020095759748.png#pic_center"><br>根据上述表达式，当窗口处在平坦区域上滑动，可以想象的到，灰度不会发生变化，那么E(u,v) = 0；如果窗口处在比纹理比较丰富的区域上滑动，那么灰度变化会很大。算法最终思想就是计算灰度发生较大变化时所对应的位置，当然这个较大是指针任意方向上的滑动，并非单指某个方向。</p>
<p>$I(x+u, y+v)$泰勒展开可得：</p>
<script type="math/tex; mode=display">I(x+u, y+v)=I(x, y)+I_{x} u+I_{y} v+O\left(u^{2}, v^{2}\right)</script><p>当发生微小位移时，忽略无穷小量，写成矩阵形式：</p>
<script type="math/tex; mode=display">E(u, v)=\sum_w[u, v]\left[\begin{array}{cc}{I_{x}^{2}} & {I_{x} I_{y}} \\ {I_{x} I_{y}} & {I_{y}^{2}}\end{array}\right]\left[\begin{array}{l}{u} \\ {v}\end{array}\right]=[u, v]M\left[\begin{array}{l}{u} \\ {v}\end{array}\right]</script><p>所以E(u,v)表达式可以更新为：</p>
<script type="math/tex; mode=display">E(u,v)\cong \begin{bmatrix}
u\\ v\end{bmatrix}M\begin{bmatrix}
u & 
v\end{bmatrix}</script><p>矩阵M为:</p>
<script type="math/tex; mode=display">M(x,y)=\Sigma_w \left[ \begin{matrix} I_x^2& I_xI_y \\ I_xI_y & I_y^2\end{matrix} \right]</script><p>E(u,v)是一个二次型，而由下述定理可知<br><img alt="在这里插入图片描述" data-src="https://img-blog.csdnimg.cn/20191020101744919.png#pic_center"><br>M分解可得：</p>
<script type="math/tex; mode=display">M=X\Sigma X^T = X \left[ \begin{matrix} \lambda_1& 0\\ 0& \lambda_2\end{matrix} \right] X^T</script><p>令E(u,v)=常数，我们可用一个椭圆来描绘这一函数<br><img alt="在这里插入图片描述" data-src="https://img-blog.csdnimg.cn/20191020101810715.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjkwNTE0MQ==,size_16,color_FFFFFF,t_70#pic_center"><br>椭圆的长短轴是与结构张量M的两个特征值相对应的量。通过判断的情况我们就可以区分出‘flat’，‘edge’，‘corner’这三种区域，因为最直观的印象：</p>
<ul>
<li>corner：在水平、竖直两个方向上变化均较大的点，即Ix、Iy都较大 <ul>
<li>edge ：仅在水平、或者仅在竖直方向有较大的点，即Ix和Iy只有其一较大 </li>
<li>flat   ： 在水平、竖直方向的变化量均较小的点，即Ix、Iy都较小</li>
</ul>
</li>
</ul>
<p>M是由Ix，Iy构成，它的特征值正好可以反映Ix，Iy的情况，下面我以一种更容易理解的方式来讲述椭圆的物理意义。<br><img alt="在这里插入图片描述" data-src="https://img-blog.csdnimg.cn/20191020101904206.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjkwNTE0MQ==,size_16,color_FFFFFF,t_70"><br>下图是对这三种情况窗口中的对应像素的梯度分布进行绘制：<br><img alt="在这里插入图片描述" data-src="https://img-blog.csdnimg.cn/20191020101940485.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjkwNTE0MQ==,size_16,color_FFFFFF,t_70#pic_center"><br>如果使用椭圆进行数据集表示，则绘制图示如下：</p>
<p><img alt="在这里插入图片描述" data-src="https://img-blog.csdnimg.cn/20191020103310120.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjkwNTE0MQ==,size_16,color_FFFFFF,t_70#pic_center"><br>可以得出下列结论：</p>
<blockquote>
<p>特征值都比较大时，即窗口中含有角点<br>特征值一个较大，一个较小，窗口中含有边缘<br>特征值都比较小，窗口处在平坦区域<img alt="在这里插入图片描述" data-src="https://img-blog.csdnimg.cn/20191020102013942.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjkwNTE0MQ==,size_16,color_FFFFFF,t_70#pic_center"></p>
</blockquote>
<h1 id="度量角点响应"><a href="#度量角点响应" class="headerlink" title="度量角点响应"></a>度量角点响应</h1><p>特征值计算一般比较繁琐，所以把M写为：</p>
<script type="math/tex; mode=display">M(x,y)=\Sigma_w \left[ \begin{matrix} I_x^2& I_xI_y \\ I_xI_y & I_y^2\end{matrix} \right] = \left[ \begin{matrix} A& C\\ C& B\end{matrix} \right]</script><p>定义角点响应函数R（corner response function），采用近似的形式，α为常数，一般取0.04-0.06：</p>
<script type="math/tex; mode=display">R = detM-\alpha (traceM)^2\\
detM=\lambda_1 \lambda_2=AB-C^2\\
traceM=\lambda_1 + \lambda_2 = A+B</script><blockquote>
<p>可以通过判断R的值来判断某个点是不是角点了。</p>
<ul>
<li>角点：R为大数值整数</li>
<li>边缘：R为大数值负数</li>
<li>平坦区：绝对值R是小数值<img alt="在这里插入图片描述" data-src="https://img-blog.csdnimg.cn/20191020102127533.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjkwNTE0MQ==,size_16,color_FFFFFF,t_70#pic_center#pic_center"><h1 id="harris角点性质"><a href="#harris角点性质" class="headerlink" title="harris角点性质"></a>harris角点性质</h1>参数α对角点检测的影响：增大α的值，将减小角点响应值R，减少被检测角点的数量；减小α的值，将增大角点响应值R，增加被检测角点的数量。</li>
</ul>
</blockquote>
<ul>
<li>Harris角点检测对亮度和对比度的变化不敏感。<br><img alt="在这里插入图片描述" data-src="https://img-blog.csdnimg.cn/20191020104945973.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjkwNTE0MQ==,size_16,color_FFFFFF,t_70#pic_center"></li>
<li>Harris角点检测具有旋转不变性，不具备尺度不变性。如下图所示，在小尺度下的角点被放大后可能会被认为是图像边缘。<br><img alt="在这里插入图片描述" data-src="https://img-blog.csdnimg.cn/20191020105015840.png#pic_center"></li>
</ul>
<h1 id="实践"><a href="#实践" class="headerlink" title="实践"></a>实践</h1><p>harris角点检测算法步骤</p>
<ol>
<li><p>利用Soble计算出XY方向的梯度值</p>
</li>
<li><p>计算出$I_x^2,I_y^2,I_xI_y$</p>
</li>
<li><p>利用高斯函数对$I_x^2,I_y^2,I_xI_y$进行滤波</p>
</li>
<li><p>计算局部特征结果矩阵M的特征值和响应函数$C(i,j)=Det(M)-k(trace(M))^2   (0.04&lt;=k&lt;=0.06)$</p>
</li>
<li><p>将计算出响应函数的值C进行非极大值抑制，滤除一些不是角点的点，同时要满足大于设定的阈值</p>
</li>
</ol>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">#include "opencv2/imgproc/imgproc.hpp"  </span></span><br><span class="line"><span class="comment">#include "opencv2/highgui/highgui.hpp"  </span></span><br><span class="line"><span class="comment">#include &lt;iostream&gt;  </span></span><br><span class="line"><span class="comment">#include &lt;cmath&gt;</span></span><br><span class="line">using namespace cv;</span><br><span class="line">using namespace std;</span><br><span class="line">/*</span><br><span class="line">RGB转换成灰度图像的一个常用公式是：</span><br><span class="line">Gray = R*0.299 + G*0.587 + B*0.114</span><br><span class="line">*/</span><br><span class="line">//******************灰度转换函数*************************  </span><br><span class="line">//第一个参数image输入的彩色RGB图像的引用；  </span><br><span class="line">//第二个参数imageGray是转换后输出的灰度图像的引用；  </span><br><span class="line">//*******************************************************</span><br><span class="line">void ConvertRGB2GRAY(const Mat &amp;image, Mat &amp;imageGray);</span><br><span class="line"> </span><br><span class="line">//******************Sobel卷积因子计算X、Y方向梯度和梯度方向角********************  </span><br><span class="line">//第一个参数imageSourc原始灰度图像；  </span><br><span class="line">//第二个参数imageSobelX是X方向梯度图像；  </span><br><span class="line">//第三个参数imageSobelY是Y方向梯度图像；  </span><br><span class="line">//第四个参数pointDrection是梯度方向角数组指针  </span><br><span class="line">//*************************************************************  </span><br><span class="line">void SobelGradDirction(Mat &amp;imageSource, Mat &amp;imageSobelX, Mat &amp;imageSobelY);</span><br><span class="line"> </span><br><span class="line">//******************计算Sobel的X方向梯度幅值的平方*************************  </span><br><span class="line">//第一个参数imageGradX是X方向梯度图像；    </span><br><span class="line">//第二个参数SobelAmpXX是输出的X方向梯度图像的平方  </span><br><span class="line">//*************************************************************  </span><br><span class="line">void SobelXX(const Mat imageGradX, Mat_&lt;<span class="built_in">float</span>&gt; &amp;SobelAmpXX);</span><br><span class="line"> </span><br><span class="line">//******************计算Sobel的Y方向梯度幅值的平方*************************    </span><br><span class="line">//第一个参数imageGradY是Y方向梯度图像；  </span><br><span class="line">//第二个参数SobelAmpXX是输出的Y方向梯度图像的平方  </span><br><span class="line">//*************************************************************  </span><br><span class="line">void SobelYY(const Mat imageGradY, Mat_&lt;<span class="built_in">float</span>&gt; &amp;SobelAmpYY);</span><br><span class="line"> </span><br><span class="line">//******************计算Sobel的XY方向梯度幅值的乘积*************************    </span><br><span class="line">//第一个参数imageGradX是X方向梯度图像；</span><br><span class="line">//第二个参数imageGradY是Y方向梯度图像；</span><br><span class="line">//第二个参数SobelAmpXY是输出的XY方向梯度图像 </span><br><span class="line">//*************************************************************  </span><br><span class="line">void SobelXY(const Mat imageGradX, const Mat imageGradY, Mat_&lt;<span class="built_in">float</span>&gt; &amp;SobelAmpXY);</span><br><span class="line"> </span><br><span class="line">//****************计算一维高斯的权值数组*****************</span><br><span class="line">//第一个参数size是代表的卷积核的边长的大小</span><br><span class="line">//第二个参数sigma表示的是sigma的大小</span><br><span class="line">//*******************************************************</span><br><span class="line">double *getOneGuassionArray(int size, double sigma);</span><br><span class="line"> </span><br><span class="line">//****************高斯滤波函数的实现*****************</span><br><span class="line">//第一个参数srcImage是代表的输入的原图</span><br><span class="line">//第二个参数dst表示的是输出的图</span><br><span class="line">//第三个参数size表示的是卷积核的边长的大小</span><br><span class="line">//*******************************************************</span><br><span class="line">void MyGaussianBlur(Mat_&lt;<span class="built_in">float</span>&gt; &amp;srcImage, Mat_&lt;<span class="built_in">float</span>&gt; &amp;dst, int size);</span><br><span class="line"> </span><br><span class="line">//****计算局部特涨结果矩阵M的特征值和响应函数H = (A*B - C) - k*(A+B)^2******</span><br><span class="line">//M</span><br><span class="line">//A  C</span><br><span class="line">//C  B</span><br><span class="line">//Tr(M)=a+b=A+B</span><br><span class="line">//Det(M)=a*b=A*B-C^2</span><br><span class="line">//计算输出响应函数的值得矩阵</span><br><span class="line">//****************************************************************************</span><br><span class="line">void harrisResponse(Mat_&lt;<span class="built_in">float</span>&gt; &amp;GaussXX, Mat_&lt;<span class="built_in">float</span>&gt; &amp;GaussYY, Mat_&lt;<span class="built_in">float</span>&gt; &amp;GaussXY, Mat_&lt;<span class="built_in">float</span>&gt; &amp;resultData,<span class="built_in">float</span> k);</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">//***********非极大值抑制和满足阈值及某邻域内的局部极大值为角点**************</span><br><span class="line">//第一个参数是响应函数的矩阵</span><br><span class="line">//第二个参数是输入的灰度图像</span><br><span class="line">//第三个参数表示的是输出的角点检测到的结果图</span><br><span class="line">void LocalMaxValue(Mat_&lt;<span class="built_in">float</span>&gt; &amp;resultData, Mat &amp;srcGray, Mat &amp;ResultImage,int kSize);</span><br><span class="line"> </span><br><span class="line">int main()</span><br><span class="line">&#123;</span><br><span class="line">	const Mat srcImage = imread(<span class="string">"3.jpg"</span>);</span><br><span class="line">	<span class="keyword">if</span> (!srcImage.data)</span><br><span class="line">	&#123;</span><br><span class="line">		<span class="built_in">printf</span>(<span class="string">"could not load image...\n"</span>);</span><br><span class="line">		<span class="built_in">return</span> -1;</span><br><span class="line">	&#125;</span><br><span class="line">	imshow(<span class="string">"srcImage"</span>, srcImage);</span><br><span class="line">	Mat srcGray;</span><br><span class="line">	ConvertRGB2GRAY(srcImage, srcGray);</span><br><span class="line">	Mat imageSobelX;</span><br><span class="line">	Mat imageSobelY;</span><br><span class="line">	Mat resultImage;</span><br><span class="line">	Mat_&lt;<span class="built_in">float</span>&gt; imageSobelXX;</span><br><span class="line">	Mat_&lt;<span class="built_in">float</span>&gt; imageSobelYY;</span><br><span class="line">	Mat_&lt;<span class="built_in">float</span>&gt; imageSobelXY;</span><br><span class="line">	Mat_&lt;<span class="built_in">float</span>&gt; GaussianXX;</span><br><span class="line">	Mat_&lt;<span class="built_in">float</span>&gt; GaussianYY;</span><br><span class="line">	Mat_&lt;<span class="built_in">float</span>&gt; GaussianXY;</span><br><span class="line">	Mat_&lt;<span class="built_in">float</span>&gt; HarrisRespond;</span><br><span class="line">	//计算Soble的XY梯度</span><br><span class="line">	SobelGradDirction(srcGray, imageSobelX, imageSobelY);</span><br><span class="line">	//计算X方向的梯度的平方</span><br><span class="line">	SobelXX(imageSobelX, imageSobelXX);</span><br><span class="line">	SobelYY(imageSobelY, imageSobelYY);</span><br><span class="line">	SobelXY(imageSobelX, imageSobelY, imageSobelXY);</span><br><span class="line">	//计算高斯模糊XX YY XY</span><br><span class="line">	MyGaussianBlur(imageSobelXX, GaussianXX,3);</span><br><span class="line">	MyGaussianBlur(imageSobelYY, GaussianYY, 3);</span><br><span class="line">	MyGaussianBlur(imageSobelXY, GaussianXY, 3);</span><br><span class="line">	harrisResponse(GaussianXX, GaussianYY, GaussianXY, HarrisRespond, 0.05);</span><br><span class="line">	LocalMaxValue(HarrisRespond, srcGray, resultImage, 3);</span><br><span class="line">	imshow(<span class="string">"imageSobelX"</span>, imageSobelX);</span><br><span class="line">	imshow(<span class="string">"imageSobelY"</span>, imageSobelY);</span><br><span class="line">	imshow(<span class="string">"resultImage"</span>, resultImage);</span><br><span class="line">	waitKey(0);</span><br><span class="line">	<span class="built_in">return</span> 0;</span><br><span class="line">&#125;</span><br><span class="line">void ConvertRGB2GRAY(const Mat &amp;image, Mat &amp;imageGray)</span><br><span class="line">&#123;</span><br><span class="line">	<span class="keyword">if</span> (!image.data || image.channels() != 3)</span><br><span class="line">	&#123;</span><br><span class="line">		<span class="built_in">return</span>;</span><br><span class="line">	&#125;</span><br><span class="line">	//创建一张单通道的灰度图像</span><br><span class="line">	imageGray = Mat::zeros(image.size(), CV_8UC1);</span><br><span class="line">	//取出存储图像像素的数组的指针</span><br><span class="line">	uchar *pointImage = image.data;</span><br><span class="line">	uchar *pointImageGray = imageGray.data;</span><br><span class="line">	//取出图像每行所占的字节数</span><br><span class="line">	size_t stepImage = image.step;</span><br><span class="line">	size_t stepImageGray = imageGray.step;</span><br><span class="line">	<span class="keyword">for</span> (int i = 0; i &lt; imageGray.rows; i++)</span><br><span class="line">	&#123;</span><br><span class="line">		<span class="keyword">for</span> (int j = 0; j &lt; imageGray.cols; j++)</span><br><span class="line">		&#123;</span><br><span class="line">			pointImageGray[i*stepImageGray + j] = (uchar)(0.114*pointImage[i*stepImage + 3 * j] + 0.587*pointImage[i*stepImage + 3 * j + 1] + 0.299*pointImage[i*stepImage + 3 * j + 2]);</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">//存储梯度膜长</span><br><span class="line">void SobelGradDirction(Mat &amp;imageSource, Mat &amp;imageSobelX, Mat &amp;imageSobelY)</span><br><span class="line">&#123;</span><br><span class="line">	imageSobelX = Mat::zeros(imageSource.size(), CV_32SC1);</span><br><span class="line">	imageSobelY = Mat::zeros(imageSource.size(), CV_32SC1);</span><br><span class="line">	//取出原图和X和Y梯度图的数组的首地址</span><br><span class="line">	uchar *P = imageSource.data;</span><br><span class="line">	uchar *PX = imageSobelX.data;</span><br><span class="line">	uchar *PY = imageSobelY.data;</span><br><span class="line"> </span><br><span class="line">	//取出每行所占据的字节数</span><br><span class="line">	int step = imageSource.step;</span><br><span class="line">	int stepXY = imageSobelX.step;</span><br><span class="line"> </span><br><span class="line">	int index = 0;//梯度方向角的索引</span><br><span class="line">	<span class="keyword">for</span> (int i = 1; i &lt; imageSource.rows - 1; ++i)</span><br><span class="line">	&#123;</span><br><span class="line">		<span class="keyword">for</span> (int j = 1; j &lt; imageSource.cols - 1; ++j)</span><br><span class="line">		&#123;</span><br><span class="line">			//通过指针遍历图像上每一个像素   </span><br><span class="line">			double gradY = P[(i + 1)*step + j - 1] + P[(i + 1)*step + j] * 2 + P[(i + 1)*step + j + 1] - P[(i - 1)*step + j - 1] - P[(i - 1)*step + j] * 2 - P[(i - 1)*step + j + 1];</span><br><span class="line">			PY[i*stepXY + j*(stepXY / step)] = abs(gradY);</span><br><span class="line"> </span><br><span class="line">			double gradX = P[(i - 1)*step + j + 1] + P[i*step + j + 1] * 2 + P[(i + 1)*step + j + 1] - P[(i - 1)*step + j - 1] - P[i*step + j - 1] * 2 - P[(i + 1)*step + j - 1];</span><br><span class="line">			PX[i*stepXY + j*(stepXY / step)] = abs(gradX);</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">	//将梯度数组转换成8位无符号整型</span><br><span class="line">	convertScaleAbs(imageSobelX, imageSobelX);</span><br><span class="line">	convertScaleAbs(imageSobelY, imageSobelY);</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">void SobelXX(const Mat imageGradX, Mat_&lt;<span class="built_in">float</span>&gt; &amp;SobelAmpXX)</span><br><span class="line">&#123;</span><br><span class="line">	SobelAmpXX = Mat_&lt;<span class="built_in">float</span>&gt;(imageGradX.size(), CV_32FC1);</span><br><span class="line">	<span class="keyword">for</span> (int i = 0; i &lt; SobelAmpXX.rows; i++)</span><br><span class="line">	&#123;</span><br><span class="line">		<span class="keyword">for</span> (int j = 0; j &lt; SobelAmpXX.cols; j++)</span><br><span class="line">		&#123;</span><br><span class="line">			SobelAmpXX.at&lt;<span class="built_in">float</span>&gt;(i, j) = imageGradX.at&lt;uchar&gt;(i, j)*imageGradX.at&lt;uchar&gt;(i, j);</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">	//convertScaleAbs(SobelAmpXX, SobelAmpXX);</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line">void SobelYY(const Mat imageGradY, Mat_&lt;<span class="built_in">float</span>&gt; &amp;SobelAmpYY)</span><br><span class="line">&#123;</span><br><span class="line">	SobelAmpYY = Mat_&lt;<span class="built_in">float</span>&gt;(imageGradY.size(), CV_32FC1);</span><br><span class="line">	<span class="keyword">for</span> (int i = 0; i &lt; SobelAmpYY.rows; i++)</span><br><span class="line">	&#123;</span><br><span class="line">		<span class="keyword">for</span> (int j = 0; j &lt; SobelAmpYY.cols; j++)</span><br><span class="line">		&#123;</span><br><span class="line">			SobelAmpYY.at&lt;<span class="built_in">float</span>&gt;(i, j) = imageGradY.at&lt;uchar&gt;(i, j)*imageGradY.at&lt;uchar&gt;(i, j);</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">	//convertScaleAbs(SobelAmpYY, SobelAmpYY);</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line">void SobelXY(const Mat imageGradX, const Mat imageGradY, Mat_&lt;<span class="built_in">float</span>&gt; &amp;SobelAmpXY)</span><br><span class="line">&#123;</span><br><span class="line">	SobelAmpXY = Mat_&lt;<span class="built_in">float</span>&gt;(imageGradX.size(), CV_32FC1);</span><br><span class="line">	<span class="keyword">for</span> (int i = 0; i &lt; SobelAmpXY.rows; i++)</span><br><span class="line">	&#123;</span><br><span class="line">		<span class="keyword">for</span> (int j = 0; j &lt; SobelAmpXY.cols; j++)</span><br><span class="line">		&#123;</span><br><span class="line">			SobelAmpXY.at&lt;<span class="built_in">float</span>&gt;(i, j) = imageGradX.at&lt;uchar&gt;(i, j)*imageGradY.at&lt;uchar&gt;(i, j);</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">	//convertScaleAbs(SobelAmpXY, SobelAmpXY);</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">//计算一维高斯的权值数组</span><br><span class="line">double *getOneGuassionArray(int size, double sigma)</span><br><span class="line">&#123;</span><br><span class="line">	double sum = 0.0;</span><br><span class="line">	//定义高斯核半径</span><br><span class="line">	int kerR = size / 2;</span><br><span class="line"> </span><br><span class="line">	//建立一个size大小的动态一维数组</span><br><span class="line">	double *arr = new double[size];</span><br><span class="line">	<span class="keyword">for</span> (int i = 0; i &lt; size; i++)</span><br><span class="line">	&#123;</span><br><span class="line"> </span><br><span class="line">		// 高斯函数前的常数可以不用计算，会在归一化的过程中给消去</span><br><span class="line">		arr[i] = exp(-((i - kerR)*(i - kerR)) / (2 * sigma*sigma));</span><br><span class="line">		sum += arr[i];//将所有的值进行相加</span><br><span class="line"> </span><br><span class="line">	&#125;</span><br><span class="line">	//进行归一化	</span><br><span class="line">	<span class="keyword">for</span> (int i = 0; i &lt; size; i++)</span><br><span class="line">	&#123;</span><br><span class="line">		arr[i] /= sum;</span><br><span class="line">		cout &lt;&lt; arr[i] &lt;&lt; endl;</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="built_in">return</span> arr;</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line">void MyGaussianBlur(Mat_&lt;<span class="built_in">float</span>&gt; &amp;srcImage, Mat_&lt;<span class="built_in">float</span>&gt; &amp;dst, int size)</span><br><span class="line">&#123;</span><br><span class="line">	CV_Assert(srcImage.channels() == 1 || srcImage.channels() == 3); // 只处理单通道或者三通道图像</span><br><span class="line">	int kerR = size / 2;</span><br><span class="line">	dst = srcImage.clone();</span><br><span class="line">	int channels = dst.channels();</span><br><span class="line">	double* arr;</span><br><span class="line">	arr = getOneGuassionArray(size, 1);//先求出高斯数组</span><br><span class="line"> </span><br><span class="line">									   //遍历图像 水平方向的卷积</span><br><span class="line">	<span class="keyword">for</span> (int i = kerR; i &lt; dst.rows - kerR; i++)</span><br><span class="line">	&#123;</span><br><span class="line">		<span class="keyword">for</span> (int j = kerR; j &lt; dst.cols - kerR; j++)</span><br><span class="line">		&#123;</span><br><span class="line">			<span class="built_in">float</span> GuassionSum[3] = &#123; 0 &#125;;</span><br><span class="line">			//滑窗搜索完成高斯核平滑</span><br><span class="line">			<span class="keyword">for</span> (int k = -kerR; k &lt;= kerR; k++)</span><br><span class="line">			&#123;</span><br><span class="line"> </span><br><span class="line">				<span class="keyword">if</span> (channels == 1)//如果只是单通道</span><br><span class="line">				&#123;</span><br><span class="line">					GuassionSum[0] += arr[kerR + k] * dst.at&lt;<span class="built_in">float</span>&gt;(i, j + k);//行不变，列变换，先做水平方向的卷积</span><br><span class="line">				&#125;</span><br><span class="line">				<span class="keyword">else</span> <span class="keyword">if</span> (channels == 3)//如果是三通道的情况</span><br><span class="line">				&#123;</span><br><span class="line">					Vec3f bgr = dst.at&lt;Vec3f&gt;(i, j + k);</span><br><span class="line">					auto a = arr[kerR + k];</span><br><span class="line">					GuassionSum[0] += a*bgr[0];</span><br><span class="line">					GuassionSum[1] += a*bgr[1];</span><br><span class="line">					GuassionSum[2] += a*bgr[2];</span><br><span class="line">				&#125;</span><br><span class="line">			&#125;</span><br><span class="line">			<span class="keyword">for</span> (int k = 0; k &lt; channels; k++)</span><br><span class="line">			&#123;</span><br><span class="line">				<span class="keyword">if</span> (GuassionSum[k] &lt; 0)</span><br><span class="line">					GuassionSum[k] = 0;</span><br><span class="line">				<span class="keyword">else</span> <span class="keyword">if</span> (GuassionSum[k] &gt; 255)</span><br><span class="line">					GuassionSum[k] = 255;</span><br><span class="line">			&#125;</span><br><span class="line">			<span class="keyword">if</span> (channels == 1)</span><br><span class="line">				dst.at&lt;<span class="built_in">float</span>&gt;(i, j) = static_cast&lt;<span class="built_in">float</span>&gt;(GuassionSum[0]);</span><br><span class="line">			<span class="keyword">else</span> <span class="keyword">if</span> (channels == 3)</span><br><span class="line">			&#123;</span><br><span class="line">				Vec3f bgr = &#123; static_cast&lt;<span class="built_in">float</span>&gt;(GuassionSum[0]), static_cast&lt;<span class="built_in">float</span>&gt;(GuassionSum[1]), static_cast&lt;<span class="built_in">float</span>&gt;(GuassionSum[2]) &#125;;</span><br><span class="line">				dst.at&lt;Vec3f&gt;(i, j) = bgr;</span><br><span class="line">			&#125;</span><br><span class="line"> </span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line"> </span><br><span class="line">	//竖直方向</span><br><span class="line">	<span class="keyword">for</span> (int i = kerR; i &lt; dst.rows - kerR; i++)</span><br><span class="line">	&#123;</span><br><span class="line">		<span class="keyword">for</span> (int j = kerR; j &lt; dst.cols - kerR; j++)</span><br><span class="line">		&#123;</span><br><span class="line">			<span class="built_in">float</span> GuassionSum[3] = &#123; 0 &#125;;</span><br><span class="line">			//滑窗搜索完成高斯核平滑</span><br><span class="line">			<span class="keyword">for</span> (int k = -kerR; k &lt;= kerR; k++)</span><br><span class="line">			&#123;</span><br><span class="line"> </span><br><span class="line">				<span class="keyword">if</span> (channels == 1)//如果只是单通道</span><br><span class="line">				&#123;</span><br><span class="line">					GuassionSum[0] += arr[kerR + k] * dst.at&lt;<span class="built_in">float</span>&gt;(i + k, j);//行变，列不换，再做竖直方向的卷积</span><br><span class="line">				&#125;</span><br><span class="line">				<span class="keyword">else</span> <span class="keyword">if</span> (channels == 3)//如果是三通道的情况</span><br><span class="line">				&#123;</span><br><span class="line">					Vec3f bgr = dst.at&lt;Vec3f&gt;(i + k, j);</span><br><span class="line">					auto a = arr[kerR + k];</span><br><span class="line">					GuassionSum[0] += a*bgr[0];</span><br><span class="line">					GuassionSum[1] += a*bgr[1];</span><br><span class="line">					GuassionSum[2] += a*bgr[2];</span><br><span class="line">				&#125;</span><br><span class="line">			&#125;</span><br><span class="line">			<span class="keyword">for</span> (int k = 0; k &lt; channels; k++)</span><br><span class="line">			&#123;</span><br><span class="line">				<span class="keyword">if</span> (GuassionSum[k] &lt; 0)</span><br><span class="line">					GuassionSum[k] = 0;</span><br><span class="line">				<span class="keyword">else</span> <span class="keyword">if</span> (GuassionSum[k] &gt; 255)</span><br><span class="line">					GuassionSum[k] = 255;</span><br><span class="line">			&#125;</span><br><span class="line">			<span class="keyword">if</span> (channels == 1)</span><br><span class="line">				dst.at&lt;<span class="built_in">float</span>&gt;(i, j) = static_cast&lt;<span class="built_in">float</span>&gt;(GuassionSum[0]);</span><br><span class="line">			<span class="keyword">else</span> <span class="keyword">if</span> (channels == 3)</span><br><span class="line">			&#123;</span><br><span class="line">				Vec3f bgr = &#123; static_cast&lt;<span class="built_in">float</span>&gt;(GuassionSum[0]), static_cast&lt;<span class="built_in">float</span>&gt;(GuassionSum[1]), static_cast&lt;<span class="built_in">float</span>&gt;(GuassionSum[2]) &#125;;</span><br><span class="line">				dst.at&lt;Vec3f&gt;(i, j) = bgr;</span><br><span class="line">			&#125;</span><br><span class="line"> </span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">	delete[] arr;</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line">void harrisResponse(Mat_&lt;<span class="built_in">float</span>&gt; &amp;GaussXX, Mat_&lt;<span class="built_in">float</span>&gt; &amp;GaussYY, Mat_&lt;<span class="built_in">float</span>&gt; &amp;GaussXY, Mat_&lt;<span class="built_in">float</span>&gt; &amp;resultData,<span class="built_in">float</span> k)</span><br><span class="line">&#123;</span><br><span class="line">	//创建一张响应函数输出的矩阵</span><br><span class="line">	resultData = Mat_&lt;<span class="built_in">float</span>&gt;(GaussXX.size(), CV_32FC1);</span><br><span class="line">	<span class="keyword">for</span> (int i = 0; i &lt; resultData.rows; i++)</span><br><span class="line">	&#123;</span><br><span class="line">		<span class="keyword">for</span> (int j = 0; j &lt; resultData.cols; j++)</span><br><span class="line">		&#123;</span><br><span class="line">			<span class="built_in">float</span> a = GaussXX.at&lt;<span class="built_in">float</span>&gt;(i, j);</span><br><span class="line">			<span class="built_in">float</span> b = GaussYY.at&lt;<span class="built_in">float</span>&gt;(i, j);</span><br><span class="line">			<span class="built_in">float</span> c = GaussXY.at&lt;<span class="built_in">float</span>&gt;(i, j);</span><br><span class="line">			resultData.at&lt;<span class="built_in">float</span>&gt;(i, j) = a*b - c*c - k*(a + b)*(a + b);</span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">//非极大值抑制</span><br><span class="line">void LocalMaxValue(Mat_&lt;<span class="built_in">float</span>&gt; &amp;resultData, Mat &amp;srcGray, Mat &amp;ResultImage, int kSize)</span><br><span class="line">&#123;</span><br><span class="line">	int r = kSize / 2;</span><br><span class="line">	ResultImage = srcGray.clone();</span><br><span class="line">	<span class="keyword">for</span> (int i = r; i &lt; ResultImage.rows - r; i++)</span><br><span class="line">	&#123;</span><br><span class="line">		<span class="keyword">for</span> (int j = r; j &lt; ResultImage.cols - r; j++)</span><br><span class="line">		&#123;</span><br><span class="line">			<span class="keyword">if</span> (resultData.at&lt;<span class="built_in">float</span>&gt;(i, j) &gt; resultData.at&lt;<span class="built_in">float</span>&gt;(i - 1, j - 1) &amp;&amp;</span><br><span class="line">				resultData.at&lt;<span class="built_in">float</span>&gt;(i, j) &gt; resultData.at&lt;<span class="built_in">float</span>&gt;(i - 1, j) &amp;&amp;</span><br><span class="line">				resultData.at&lt;<span class="built_in">float</span>&gt;(i, j) &gt; resultData.at&lt;<span class="built_in">float</span>&gt;(i - 1, j - 1) &amp;&amp;</span><br><span class="line">				resultData.at&lt;<span class="built_in">float</span>&gt;(i, j) &gt; resultData.at&lt;<span class="built_in">float</span>&gt;(i - 1, j + 1) &amp;&amp;</span><br><span class="line">				resultData.at&lt;<span class="built_in">float</span>&gt;(i, j) &gt; resultData.at&lt;<span class="built_in">float</span>&gt;(i, j - 1) &amp;&amp;</span><br><span class="line">				resultData.at&lt;<span class="built_in">float</span>&gt;(i, j) &gt; resultData.at&lt;<span class="built_in">float</span>&gt;(i, j + 1) &amp;&amp;</span><br><span class="line">				resultData.at&lt;<span class="built_in">float</span>&gt;(i, j) &gt; resultData.at&lt;<span class="built_in">float</span>&gt;(i + 1, j - 1) &amp;&amp;</span><br><span class="line">				resultData.at&lt;<span class="built_in">float</span>&gt;(i, j) &gt; resultData.at&lt;<span class="built_in">float</span>&gt;(i + 1, j) &amp;&amp;</span><br><span class="line">				resultData.at&lt;<span class="built_in">float</span>&gt;(i, j) &gt; resultData.at&lt;<span class="built_in">float</span>&gt;(i + 1, j + 1))</span><br><span class="line">			&#123;</span><br><span class="line">				<span class="keyword">if</span> ((int)resultData.at&lt;<span class="built_in">float</span>&gt;(i, j) &gt; 18000)</span><br><span class="line">				&#123;</span><br><span class="line">					circle(ResultImage, Point(i, j), 5, Scalar(0,0,255), 2, 8, 0);</span><br><span class="line">				&#125;</span><br><span class="line">			&#125;</span><br><span class="line"> </span><br><span class="line">		&#125;</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>参考：<a href="https://blog.csdn.net/linqianbi/article/details/78930239" target="_blank" rel="noopener">https://blog.csdn.net/linqianbi/article/details/78930239</a></p>
]]></content>
      <categories>
        <category>VINS</category>
      </categories>
  </entry>
  <entry>
    <title>【VINS-Mono】A Robust and Versatile Monocular Visual-Inertial State Estimator</title>
    <url>/2019/10/19/%E3%80%90VINS%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91%E3%80%91VINS-Mono-A-Robust-and-Versatile-Monocular-Visual-Inertial-State-Estimator/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><font color="gray" size="10"><center>VINS-Mono：一种鲁棒且通用的单目视觉惯性状态估计器</center></font><p><strong>摘要</strong>：由一个相机和一个低成本惯性测量单元(IMU)组成的单目视觉惯性系统(VINS)，构成了用于度量六自由度状态估计的最小传感器套件。然而，由于缺乏直接距离测量，在IMU处理、估计器初始化、外部标定和非线性优化等方面提出了重大挑战。本文提出了VINS-Mono：一种具有鲁棒性和通用性的单目视觉惯性状态估计器。该方法从用于估计器初始化和故障恢复的鲁棒的程序开始。采用一种基于紧耦合、非线性优化的方法，通过融合预积分后的IMU测量值和特征观测值，获得高精度的视觉惯性里程计。结合紧耦合方法，回环检测模块能够以最小的计算代价实现重定位。此外，我们还进行四自由度位姿图优化，以加强全局一致性。我们验证了该系统在公共数据集和真实环境实验的性能，并与其他最先进的算法进行了比较。我们还在MAV平台上执行机载闭环自主飞行，并将算法移植到基于iOS的demo中。特别强调的是，本文提出的工作是一个可靠、完整和通用的系统，适用于需要高精度定位的不同应用。我们为PC和iOS移动设备开源了我们的实现方法。<br>关键词：单目视觉惯性系统，状态估计，传感器融合，SLAM</p><a id="more"></a>


<h1 id="I-引言"><a href="#I-引言" class="headerlink" title="I. 引言"></a>I. 引言</h1><p>状态估计无疑是机器人导航、自主驾驶、虚拟现实(VR)和增强现实(AR)等广泛应用中最基本的模块。仅使用单目摄像机的方法由于其体积小、成本低和硬件设置简单而获得了社会的极大兴趣[1]-[5]。然而，单目视觉系统无法恢复度量尺度，因此限制了它们在实际机器人中的应用。近年来，我们看到了一种发展趋势，即用低成本惯性测量单元(IMU)辅助单目视觉系统。这种单目视觉-惯性系统(VINS)的主要优点是具有可观测的度量尺度，以及翻滚角(roll)和俯仰角(pitch)。这让需要有尺度的状态估计的导航任务成为可能。此外，==对IMU测量值的积分可以显著提高运动跟踪性能，弥补光照变化、缺少纹理的区域或运动模糊的视觉轨迹损失的差距==。事实上，单目VINS不仅广泛应用于移动机器人、无人机和移动设备上，还是满足充分自我感知和环境感知的最小传感器。</p>
<p>然而，所有这些优点都是有代价的。对于单目VINS，众所周知，需要加速度激励以测量尺度。这意味着==单目VIN估计器不能从静止状态启动，而是从未知的移动状态发动==。同时要认识到视觉惯性系统高度非线性的事实，在估计器初始化方面还有重大挑战。两个传感器的存在也使得摄像机-IMU的外部校准至关重要。最后，为了消除在可接受的处理窗口内的长期漂移，提出了一个完整的系统，包括视觉惯性里程计、回环检测、重定位和全局优化。</p>
<p>为了解决所有这些问题，我们提出了VINS-Mono，一个鲁棒且通用的单目视觉惯性状态估计器。我们的解决方案开始于即时估计初始化。这个初始化模块也用于故障恢复。我们的解决方案的核心是一个鲁棒的基于紧耦合的滑动窗非线性优化的单目视觉惯性里程计(VIO)。==单目VIO模块不仅提供精确的局部姿态、速度和方位估计，而且还以在线方式执行摄像机IMU外部校准和IMU偏置校正==。使用DBoW2[6]进行回环检测。==重新定位是在对单目VIO进行特征级别融合的紧耦合设置中完成==。这使得重新定位具有鲁棒性和精确性且有最小的计算代价。最后，几何验证的回环被添加到位姿图中，并且由于来自单目VIO的可观测的翻滚角和俯仰角，生成四自由度(DOF)位姿图以确保全局一致性。</p>
<p>VINS-Mono结合并改进了我们先前在单目视觉-惯性融合方面的工作[7]-[10]。它建立在我们紧耦合、基于优化的单目VIO的公式之上[7][8]，并结合了[9]中引入的改进初始化过程。[10]中给出了移植到移动设备的第一次尝试。与我们以前的工作相比，VINS-Mono的进一步改进包括改进的含偏置校正的IMU预积分、紧耦合重定位、全局位姿图优化、广泛的实验评估以及鲁棒和通用的开源实现。</p>
<p>整个系统完整且易于使用。它已经被成功应用于小规模AR场景、中型无人机导航和大规模状态估计任务。与其他最先进的方法相比具有优异的性能。为此，我们总结了我们的贡献，如下所示：<br>1、一个鲁棒的初始化过程，它能够从未知的初始状态引导系统。<br>2、一个紧耦合、基于优化的单目视觉惯性里程计，具有相机-IMU外部校准和IMU偏置估计。<br>3、在线回环检测与紧耦合重定位。<br>4、四自由度全局位姿图优化。<br>5、用于无人机导航、大规模定位和移动AR应用的实时性能演示。<br>6、完全集成于ros的pc版本以及可在iphone 6或更高版本上运行的IOS版本的开源代码。</p>
<p>论文的其余部分如下：在第二节中，我们讨论了相关的文献。在第三节中，我们对完整的系统框架进行了概述。在第四节中，给出了视觉的预处理和IMU测量值的预积分步骤。在第五节中，我们讨论了估计器的初始化过程。在第六节中提出了一种紧耦合、自标定、非线性优化的单目VIO。第七节和第八节分别给出了紧耦合重定位和全局位姿图优化。实施细节和实验结果见第九节。最后，第十节本文对研究方向进行了探讨和展望。<br><img alt="在这里插入图片描述" data-src="https://img-blog.csdnimg.cn/20191019194830365.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjkwNTE0MQ==,size_16,color_FFFFFF,t_70#pic_center"></p>
<h1 id="II-相关工作"><a href="#II-相关工作" class="headerlink" title="II. 相关工作"></a>II. 相关工作</h1><p>关于基于单目视觉的状态估计/里程计SLAM的学术工作非常广泛。值得注意的方法包括PTAM[1]、SVO[2]、LSD-SLAM[3]、DSO[5]和ORB-SLAM[4]。显然，尝试对任何方法进行全面回顾都无法完整。然而，在这一节中，我们跳过了关于只使用视觉的方法的讨论，而只专注于关于单目视觉惯性状态估计的最相关的结果。<br>处理视觉和惯性测量的最简单的方法是松耦合的传感器融合[11][12]，其中IMU被视为一个独立的模块，用于辅助运动的视觉结构(sfm)获得的纯视觉位姿估计。融合通常由扩展卡尔曼滤波(EKF)完成，其中IMU用于状态传播，而视觉位姿用于更新。进一步说，紧耦合视觉惯性算法要么基于EKF[13]-[15]，要么基于图优化[7][8][16][17]，其中相机和IMU测量值是从原始测量水平联合优化的。一种流行的基于EKF的VIO方法是MSCKF[13][14]。MSCKF在状态向量中维护以前的几个摄像机位姿，并使用多个摄像机视图中相同特征的视觉测量来形成多约束更新。SR-ISWF[18][19]是MSCKF的扩展。它采用squareroot形式[20]实现单精度表示，避免了较差的数值性质。该方法采用逆滤波器进行迭代再线性化，使其与基于优化的算法相当。批量图优化或集束调整技术（BA）维护和优化所有测量值以获得最优状态估计。为了达到恒定的处理时间，==流行的基于图的VIO方法[8][16][17]通常采用边缘化过去的状态和测量来优化最近状态的有界滑动窗口==。由于对非线性系统迭代求解的计算要求很高，很少有基于图的非线性系统能够在资源受限的平台（如手机上）实现实时性能。</p>
<p>对于视觉测量处理，根据视差模型的定义，算法可分为直接法和间接法。直接法[2][3][21]最小化光度误差，而间接法最小化几何位移。直接法因其吸引区域小，需要很好的初始估计，而间接法在提取和匹配特征时需要额外的计算资源。间接法由于其成熟性和鲁棒性，在实际工程部署中得到了广泛的应用。然而，直接法更容易扩展到稠密建图，因为它们是直接在像素级别上操作的。</p>
<p>在实践中，IMU通常以比摄像机更高的速率获取数据。不同的方法被提出来处理高速率的IMU测量值。最简单的方法是在基于EKF的方法中使用IMU进行状态传播[11][13]。在图优化公式中，为了避免重复的IMU重复积分，提出了一种有效的方法，即IMU预积分(IMU pre-integration)。这种方法在[22]中首次提出的，它用欧拉角来参数化旋转误差。在我们先前的工作中[7]，我们提出了一种流形上的IMU预积分旋转公式，利用连续IMU误差状态动力学推导了协方差传递方程。然而IMU偏置被忽略了。在[23]中通过增加后验IMU偏置校正，进一步改进了预积分理论。</p>
<p>精确的初始值对于引导任何单目VINS是至关重要的。在[8][24]中提出了一种利用短期IMU预积分相对旋转的线性估计器初始化方法。但是，该方法不对陀螺仪偏置进行建模，无法在原始投影方程中对现代传感器噪声进行建模。在实际应用中，当视觉特性远离传感器套件时，这会导致不可靠的初始化。在[25]中给出了单目视觉惯性初始化问题的一种封闭解。随后，在[26]中提出了对这种封闭形式的解决方案的扩展，增加了陀螺仪的偏置校准。这些方法依赖于长时间内IMU测量的双重积分，无法模拟惯性积分的不确定性。在[27]中，提出了一种基于SVO的重初始化和故障恢复算法。这是一种基于松耦合融合框架的实用方法。然而，需要额外的朝下的距离传感器来恢复度量尺度。在[17]中引入了一种建立在ORB-SLAM[4]上的初始化算法。给出了一组ORB-SLAM的关键帧，计算了视觉惯性全局BA的尺度、重力方向、速度和IMU偏置的初步估计。然而，尺度收敛所需的时间可能超过10秒。这可能会给需要在一开始就进行尺度估计的机器人导航任务带来问题。</p>
<p>VIO方法，不管它们所依赖的基本数学公式，在全局的平移和旋转中长期受到漂移的影响。为此，回环检测在长期操作中起着重要的作用。ORB-SLAM[4] 利用了词袋模型能够闭合回环并重用地图。回环检测之后进行7自由度（位置、方向和尺度）的位姿图优化。相对于单目VINS，由于IMU的加入，漂移只发生在4自由度，即三维平移，和围绕重力方向的旋转（偏航角）。因此，本文选择在最小四自由度设定下，优化具有回环约束的位姿图。</p>
<h1 id="III-概述"><a href="#III-概述" class="headerlink" title="III.概述"></a>III.概述</h1><p>提出的单目视觉惯性状态估计器的结构如图2所示。该系统从测量预处理(IV)开始，在其中提取和跟踪特征，对两个连续帧间的IMU测量值进行预积分。初始化过程(V)提供了所有必要的值，包括姿态、速度、重力向量、陀螺仪偏置和三维特征位置，用于引导随后的基于非线性优化的VIO。VIO(VI)与重定位(VII)模块紧密地融合了预先积分的IMU测量、特征观测和回环重新检测到的特征。最后，位姿图优化模块(VIII)接受几何验证的重定位结果，并进行全局优化以消除漂移。VIO、重新定位和位姿图优化模块在多线程设置中同时运行。每个模块有不同的运行速度和实时保证，以确保在任何时候可靠运行。<br><img alt="在这里插入图片描述" data-src="https://img-blog.csdnimg.cn/2019101920013857.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjkwNTE0MQ==,size_16,color_FFFFFF,t_70#pic_center"><br>我们现在对整篇论文中使用的符号和坐标系进行定义。我们认为(.)w(.)w是世界坐标系(world frame)。重力方向与世界坐标系z轴对齐。$(⋅)b$是本体坐标系(body frame)，我们把它定义为与IMU坐标系相同。$(⋅)c$是相机坐标系(camera frame)。我们同时使用旋转矩阵R和Hamilton四元数q来表示旋转。我们主要在状态向量中使用四元数，也用旋转矩阵来表示三维向量的旋转。$q<em>{wb}，p</em>{wb}$​表示从本体坐标系到世界坐标系的旋转和平移。$b_k$​表示获取第k个图像时的本体坐标系。$c_k$表示获取第k个图像时的相机坐标系。⊗表示两个四元数之间的乘法运算。$g_w=[0,0,g]T$是世界坐标系上的重力向量。最后，我们将(^)表示为某一具体量的噪声测量值或估计值。</p>
<h1 id="IV-测量预处理"><a href="#IV-测量预处理" class="headerlink" title="IV.测量预处理"></a>IV.测量预处理</h1><p>本节介绍VIO的预处理步骤。对于视觉测量，我们跟踪连续帧之间的特征，并在最新帧中检测新特征。对于IMU测量，我们在两个连续帧之间做预积分。请注意，我们使用的低成本IMU的测量值受到偏置和噪声的影响。因此，我们在IMU预积分过程中特别考虑偏置。</p>
<h2 id="A-视觉处理前端"><a href="#A-视觉处理前端" class="headerlink" title="A.视觉处理前端"></a>A.视觉处理前端</h2><p>对于每一幅新图像，KLT稀疏光流算法对现有特征进行跟踪[29]。同时，检测新的角点特征[30]以保证每个图像特征的最小数目(100-300)。该检测器通过设置两个相邻特征之间像素的最小间隔来执行均匀的特征分布。二维特征首先是不失真的，然后在通过外点剔除后投影到一个单位球面上。利用基本矩阵模型的RANSAC算法进行外点剔除。<br>在此步骤中还选择了关键帧。我们有两个关键帧选择标准。第一是与上一个关键帧的平均视差。如果在当前帧和最新关键帧之间跟踪的特征点的平均视差超出某个特定阈值，则将该帧视为新的关键帧。请注意，不仅平移，旋转也会产生视差。然而，特征点无法在纯旋转运动中三角化。为了避免这种情况，在计算视差时我们使用陀螺仪测量值的短时积分来补偿旋转。请注意，此旋转补偿仅用于关键帧选择，而不涉及VINS公式中的旋转计算。为此，即使陀螺仪含有较大的噪声或存在偏置，也只会导致次优的关键帧选择结果，不会直接影响估计质量。另一个标准是跟踪质量。如果跟踪的特征数量低于某一阈值，我们将此帧视为新的关键帧。这个标准是为了避免跟踪特征完全丢失。</p>
<h1 id="B-IMU预积分"><a href="#B-IMU预积分" class="headerlink" title="B.IMU预积分"></a>B.IMU预积分</h1><p>IMU预积分是在[22]中首次提出的，它将欧拉角的旋转误差参数化。在我们先前的工作中[7]，我们提出了一个流形上的IMU预积分旋转公式。该文利用连续时间的IMU误差状态动力学推导协方差传递函数，但忽略了IMU偏置。文[23]通过增加后验IMU偏置校正，进一步改进了预积分理论。本文通过引入IMU偏置校正，扩展了我们在前面工作[7]中提出的IMU预积分。<br>IMU的原始陀螺仪和加速度计测量结果$\hat w$ 和$\hat a$如下：<br><img alt="在这里插入图片描述" data-src="https://img-blog.csdnimg.cn/20191019201549992.png#pic_center"><br>IMU测量值是在本体坐标系中测量的，它是平衡重力和平台动力的合力，并受到加速度偏置ba、陀螺仪偏置bw和附加噪声的影响。假设加速度计和陀螺仪测量值中的附加噪声为高斯噪声，$n<em>a~N(0,sigma^2_a),n_w~N(0,sigma^2_w)$​。加速度计偏置和陀螺仪偏置被建模为随机游走，其导数为高斯性的，$n</em>{ba}~N(0,σ^2<em>{ba}),n</em>{bw}~N(0,σ^2<em>{bw})$。<br><img alt="在这里插入图片描述" data-src="https://img-blog.csdnimg.cn/20191019202057834.png#pic_center"><br>给定对应于体坐标系$b_k$和$b</em>{k+1}$的两个时刻，位置、速度和方向状态可以在时间间隔$[t<em>k,t</em>{k+1}]$间，在世界坐标系下中通过惯性测量值传递：<br><img alt="在这里插入图片描述" data-src="https://img-blog.csdnimg.cn/20191019202237245.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjkwNTE0MQ==,size_16,color_FFFFFF,t_70#pic_center"><br>$∆t<em>k$是时间间隔$[t_k,t</em>{k+1}]$之间的持续时间。</p>
<p>可见，IMU状态传递需要坐标系$b_k$的旋转、位置和速度。当这些起始状态改变时，我们需要重新传递IMU测量值。特别是在基于优化的算法中，每次调整位姿时，都需要在它们之间重新传递IMU测量值。这种传递策略在计算上要求很高。为了避免重新传递，我们采用了预积分算法。<br>将参考坐标系从世界坐标系转变为局部坐标系$b_k$后，我们只能对线性的加速度$\hat a$和角速度$\hat w$相关的部分进行预积分，如下所示：<br><img alt="在这里插入图片描述" data-src="https://img-blog.csdnimg.cn/20191019202449110.png#pic_center"><br><img alt="在这里插入图片描述" data-src="https://img-blog.csdnimg.cn/20191019202554843.png#pic_center"></p>
<p><img alt="在这里插入图片描述" data-src="https://img-blog.csdnimg.cn/2019101920251370.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjkwNTE0MQ==,size_16,color_FFFFFF,t_70#pic_center"><br>可以看出预积分项(6)能通过将$b<em>k$视为参考帧的IMU测量值单独得到。$α^{b_k}</em>{k+1}$、$β^{b-k}b<em>{k+1}$、$γ^{b-k}</em>{k+1}$只与$b<em>k$和$b</em>{k+1}$中的IMU偏置有关，与其他状态无关。当偏置估计发生变化时，若偏置变化很小，我们将$α^{bk}<em>{k+1}、β^{bk}b</em>{k+1}、γ^{bk}_{k+1}$按其对偏置的一阶近似来调整，否则就进行重新传递。这种策略为基于优化的算法节省了大量的计算资源，因为我们不需要重复传递IMU测量值。</p>
<p>对于离散时间的实现，可以采用不同的数值积分方法，如欧拉积分、中点积分、RK4积分等。这里选择了欧拉积分来演示易于理解的过程(我们在代码中使用了中点积分)。</p>
<p>在开始时，$\alpha^{bk}<em>{k}$、$β^{bk}</em>{k}$是0，$\gamma^{bk}<em>{k}$是单位四元数。$α$，$β$，$γ$在(6)中的平均值是如下逐步传递的。注意，增加的噪声项$n_a$，$n_w$是未知的，在实现中被视为零。这得到了预积分的估计值，标记为$(\hat⋅)$：<br><img alt="在这里插入图片描述" data-src="https://img-blog.csdnimg.cn/20191019203125870.png#pic_center"><br>i是在$[t_k,t</em>{k+1}]$中IMU测量值对应的离散时刻，$δ_t$是IMU测量值i和i+1之间的时间间隔。</p>
<p>然后讨论协方差传递问题。由于四维旋转四元数$γ^{b_k}_t$被过参数化，我们将其误差项定义为围绕其平均值的扰动：<br><img alt="在这里插入图片描述" data-src="https://img-blog.csdnimg.cn/20191019203259791.png#pic_center"><br>其中$δθ^{b_k}_t$是三维小扰动。</p>
<p>我们可以导出误差项的连续时间线性化方程(6)：<br><img alt="在这里插入图片描述" data-src="https://img-blog.csdnimg.cn/20191019203414375.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjkwNTE0MQ==,size_16,color_FFFFFF,t_70#pic_center"><br>$P^{bk}<em>{b</em>{k+1}}$可以通过初始协方差$P^{b<em>k}</em>{b<em>k}=0$的一阶离散时间协方差更新递归计算：<br><img alt="在这里插入图片描述" data-src="https://img-blog.csdnimg.cn/20191019203627976.png#pic_center"><br>其中Q是噪声的对角线协方差矩阵$(σ^2_a,σ^2_w,σ^2</em>{b<em>a},σ^2</em>{b_w})$</p>
<p>同时，$δz<em>{b</em>{k+1}}^{b<em>k}$​的一阶雅可比矩阵$J</em>{b<em>{k+1}}$​​相对于δzbk​bk​​也可以用初始雅可比矩阵$J</em>{b<em>k}=I$ 递归计算。<br><img alt="在这里插入图片描述" data-src="https://img-blog.csdnimg.cn/20191019203912837.png#pic_center"><br>利用这个递推公式，得到协方差矩阵$P^{b_k}</em>{b<em>{k+1}}$​​和雅可比矩阵$J^{b_k}</em>{b<em>{k+1}}$​​、$α^{b_k}</em>{b<em>{k+1}}$、$β^{b_k}</em>{b<em>{k+1}}$、$γ^{b_k}</em>{b<em>{k+1}}$关于偏置的一阶近似可以写为：<br><img alt="在这里插入图片描述" data-src="https://img-blog.csdnimg.cn/20191019203947220.png#pic_center"><br>其中$J^α</em>{b<em>a}$是$J_b{k+1}$中的子块矩阵，其位置对应于$\frac{δα^{b_k}</em>{b<em>{k+1}}}{δb</em>{a<em>k}}$。<br>$J^α</em>{b<em>w}、J^β</em>{b<em>a}、J^β</em>{b<em>w}、J^γ</em>{b_w}$也使用同样的含义。当偏置估计发生轻微变化时，我们使用(12)近似校正预积分结果，而不重新传递。</p>
<p>现在我们可以写下IMU测量模型所其对应的协方差$P^{b<em>k}</em>{b_{k+1}}$：<br><img alt="在这里插入图片描述" data-src="https://img-blog.csdnimg.cn/20191019204124533.png#pic_center"></p>
<h1 id="V-估计器初始化"><a href="#V-估计器初始化" class="headerlink" title="V.估计器初始化"></a>V.估计器初始化</h1><p>单目紧耦合VIO是一个高度非线性的系统。由于单目相机无法直接观测到尺度，因此，如果没有良好的初始值，很难直接将这两种测量结果融合在一起。可以假设一个静止的初始条件来启动单目VINS估计器。然而，这种假设是不合适的，因为在实际应用中经常会遇到运动下的初始化。当IMU测量结果被大偏置破坏时，情况就变得更加复杂了。事实上，初始化通常是单目VINS最脆弱的步骤。需要一个鲁棒的初始化过程以确保系统的适用性。</p>
<p>我们采用松耦合的传感器融合方法得到初始值。我们发现纯视觉SLAM，或从运动中恢复结构(SfM)，具有良好的初始化性质。在大多数情况下，纯视觉系统可以通过从相对运动方法（如八点法[32]或五点法[33]或估计单应性矩阵）中导出初始值来引导自己。通过对齐IMU预积分与纯视觉SfM结果，我们可以粗略地恢复尺度、重力、速度，甚至偏置。这足以引导非线性单目VINS估计器，如图4所示。<br><img alt="在这里插入图片描述" data-src="https://img-blog.csdnimg.cn/20191019204212487.png#pic_center"><br>与在初始阶段同时估计陀螺仪和加速度计偏置的[17]相比，我们在初始阶段选择忽略加速度计偏置项。加速度计偏置与重力耦合，且由于重力向量相对于平台动力学的大量级，以及初始阶段相对较短，这些偏置项很难被观测到。我们以前的工作对加速度计偏置标定进行了详细的分析[34]。</p>
<h1 id="A-滑动窗口-Sliding-Window-纯视觉SfM"><a href="#A-滑动窗口-Sliding-Window-纯视觉SfM" class="headerlink" title="A. 滑动窗口(Sliding Window)纯视觉SfM"></a>A. 滑动窗口(Sliding Window)纯视觉SfM</h1><p>初始化过程从纯视觉SfM估计相机尺度位姿(up-to-scale)和特征位置图开始。</p>
<p>我们保持了一个帧的滑动窗口来限制计算复杂度。首先，我们检查了最新帧与之前所有帧之间的特征对应。如果我们能在滑动窗口中的最新帧和任何其他帧之间，找到稳定的特征跟踪(超过30个跟踪特征)和足够的视差(超过20个的旋转补偿像素)，我们使用五点法[33]恢复这两个帧之间的相对旋转和尺度平移。否则，我们将最新的帧保存在窗口中，并等待新的帧。如果五点算法成功的话，我们任意设置尺度，并对这两个帧中观察到的所有特征进行三角化。基于这些三角特征，采用PnP[35]来估计窗口中所有其他帧的姿态。最后，应用全局光束平差法(BA)[36]最小化所有特征观测的重投影误差。由于我们还没有任何世界坐标系的知识，我们将第一个相机坐标系$(·)^{c<em>0}$设置为SfM的参考坐标系。所有帧的位姿$(\bar p^{c0}</em>{c<em>k}，q^{c0}</em>{c_k})$和特征位置表示相对于$(·)^{c_0}$。假设摄像机和IMU之间有一个粗略测量的外部参数$(p^b_c,q^b_c)$，我们可以将姿态从相机坐标系转换到物体(IMU)坐标系。<br><img alt="在这里插入图片描述" data-src="https://img-blog.csdnimg.cn/201910192045598.png#pic_center"><br>其中s是匹配视觉结构与距离尺度的尺度参数，解出尺度参数是实现成功初始化的关键。</p>
<h2 id="B-视觉惯性校准"><a href="#B-视觉惯性校准" class="headerlink" title="B. 视觉惯性校准"></a>B. 视觉惯性校准</h2><p>1）陀螺仪偏置标定：考虑窗口中连续两帧bk和bk+1，我们从视觉sfM中得到旋转$q^{c0}<em>{b_k}$和$q^{c0}</em>{b<em>{k+1}}$​​，从IMU预积分得到的相对约束$γ^{b_k}</em>{b<em>{k+1}}$​。我们对陀螺仪偏置求IMU预积分项的线性化，并最小化以下代价函数：<br><img alt="在这里插入图片描述" data-src="https://img-blog.csdnimg.cn/20191019204702396.png#pic_center"><br>其中B代表窗口的所有帧。利用第四部分导出的偏置雅可比，给出了$γ^{b_k}</em>{b<em>{k+1}}$​对陀螺仪偏置的一阶近似。这样，我们得到了陀螺仪偏置bw的初始校准。然后我们用新的陀螺仪偏置重新传递所有的IMU预积分项$\hat α^{b_k}</em>{b<em>{k+1}}、\hat β^{b_k}</em>{b<em>{k+1}}、\hat γ^{b_k}</em>{b_{k+1}}$ 。</p>
<p>2）速度、重力向量和尺度初始化：在陀螺仪偏置初始化后，我们继续初始化导航的其他基本状态，即速度、重力向量和尺度：<br><img alt="在这里插入图片描述" data-src="https://img-blog.csdnimg.cn/20191019204809433.png#pic_center"><br>其中，$v^{b<em>k}</em>{b_k}$​​是第k帧图像本体坐标系的速度，$g^{c_0}$是$c_0$坐标系中的重力向量，s是单目SfM到公制单位的尺度。</p>
<p>考虑窗口中两个连续帧$b<em>k$和$b</em>{k+1}$，那么(5)可以写成：<br><img alt="在这里插入图片描述" data-src="https://img-blog.csdnimg.cn/20191019205047939.png#pic_center"><br>我们可以将(14)和(17)合并成以下线性测量模型：<br><img alt="在这里插入图片描述" data-src="https://img-blog.csdnimg.cn/20191019204900731.png#pic_center"><br>可以看出，$R^{c<em>0}</em>{b<em>k}，R^{c_0}</em>{b<em>{k+1}}，\hat p^{c_0}</em>{c<em>k}，\hat p^{c_0}</em>{c_{k+1}}$是从带尺度的单目视觉中得到的，$∆t_k$是两个连续帧之间的时间间隔。通过求解线性最小二乘问题：<br><img alt="在这里插入图片描述" data-src="https://img-blog.csdnimg.cn/20191019205002105.png#pic_center"><br>我们可以得到窗口中每一帧的本体坐标系速度，视觉参照系$(·)^{c_0}$的重力向量，以及尺度参数。</p>
<p>3）重力细化：通过约束量值，可以对原线性初始化步骤得到的重力向量进行细化。在大多数情况下，重力向量的大小是已知的。这导致重力向量只剩2个自由度。因此，我们在其切线空间上用两个变量重新参数化重力。参数化将重力向量表示为<br><img alt="在这里插入图片描述" data-src="https://img-blog.csdnimg.cn/20191019205157776.png#pic_center"><br>其中g是已知的重力大小，$\bar {\hat g}$​​是表示重力方向的单位向量，b1和b2是跨越切平面的两个正交基，如图5所示，w1和w2分别是在b1和b2上的对应位移。通过算法1的叉乘运算，可以找到一组b1、b2。然后用</p>
<p><img alt="在这里插入图片描述" data-src="https://img-blog.csdnimg.cn/2019101920522547.png#pic_center"><br>代替(17)中的g，并与其它状态变量一起求解w1和w2。此过程迭代到$\bar {\hat g}$收敛为止。<br><img alt="在这里插入图片描述" data-src="https://img-blog.csdnimg.cn/20191019205247493.png#pic_center"><br><img alt="在这里插入图片描述" data-src="https://img-blog.csdnimg.cn/20191019205318309.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjkwNTE0MQ==,size_16,color_FFFFFF,t_70#pic_center"><br>4）完成初始化：经过对重力向量的细化，通过将重力旋转到z轴上，得到世界坐标系与摄像机坐标系c0之间的旋转$q^w_{c_0}$​。然后我们将所有变量从参考坐标系$(·)^{c_0}$ 旋转到世界坐标系$(·)^w$。本体坐标系的速度也将被旋转到世界坐标系。视觉SfM的变换矩阵将被缩放到度量单位。此时，初始化过程已经完成，所有这些度量值都将被输入到一个紧耦合的单目VIO中。</p>
<h1 id="VI-紧耦合单目VIO"><a href="#VI-紧耦合单目VIO" class="headerlink" title="VI. 紧耦合单目VIO"></a>VI. 紧耦合单目VIO</h1><p>在估计器初始化后，我们采用基于滑动窗口的紧耦合单目VIO进行高精度和鲁棒的状态估计。图3显示了滑动窗口的图示。<br><img alt="在这里插入图片描述" data-src="https://img-blog.csdnimg.cn/20191019205519675.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjkwNTE0MQ==,size_16,color_FFFFFF,t_70"></p>
<h2 id="A-公式"><a href="#A-公式" class="headerlink" title="A. 公式"></a>A. 公式</h2><p>滑动窗口中的完整状态向量定义为：<br><img alt="在这里插入图片描述" data-src="https://img-blog.csdnimg.cn/20191019205554259.png#pic_center"><br>其中$x_k$是捕获第k图像时的IMU状态。它包含了IMU在世界坐标系中的位置、速度和方向，以及在IMU本体坐标系中的加速度计偏置和陀螺仪偏置。n是关键帧的总数，m是滑动窗口中的特征总数，$λ_l$是第一次观测到第l个特征的逆深度。</p>
<p>我们使用视觉惯性BA。我们最小化所有测量残差的先验和Mahalanobis范数之和，得到最大后验估计：</p>
<p><img alt="在这里插入图片描述" data-src="https://img-blog.csdnimg.cn/20191019205631782.png#pic_center"><br>$r<em>B​(z^{b_k}</em>{b_{k+1}},X)$和$r_C(\hat z_l^{c_j},X)$分别是IMU和视觉测量的残差。残差的详细定义将在第六节的B和C中提出。B是所有IMU测量的集合，C是在当前滑动窗口中至少观察到两次的一组特征。${r_p,H_p}$是来自边缘化的先验信息。Ceres Solver[38]被用来解决这个非线性问题。</p>
<h2 id="B-IMU测量残差"><a href="#B-IMU测量残差" class="headerlink" title="B. IMU测量残差"></a>B. IMU测量残差</h2><p>考虑滑动窗口中连续两个帧$b<em>k$和$b</em>{k+1}$内的IMU测量，根据(13)中定义的IMU测量模型，预积分IMU测量的残差可以定义为：<br><img alt="在这里插入图片描述" data-src="https://img-blog.csdnimg.cn/20191019205738287.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjkwNTE0MQ==,size_16,color_FFFFFF,t_70#pic_center"><br>其中，$[·]<em>{xyz}$是提取四元数q的向量部分，以进行误差状态表示。$δθ^{bk}</em>{b<em>{k+1}}$​​是四元数的三维误差状态表示。$[\hat α^{b_k}</em>{b{k+1}}、\hat β^{b<em>k}</em>{b{k+1}}      \hat γ^{b<em>k}</em>{b{k+1}}]^T$是在两个连续图像帧的间隔时间内使用仅包含噪声的加速度计和陀螺仪测量值预积分的IMU测量项。加速度计和陀螺仪偏置也包括在在线校正的剩余项中。</p>
<h2 id="C-视觉测量残差"><a href="#C-视觉测量残差" class="headerlink" title="C. 视觉测量残差"></a>C. 视觉测量残差</h2><p>与在广义图像平面上定义重投影误差的传统针孔相机模型相比，我们在单位球面上定义摄像机的测量残差。几乎所有类型相机的光学，包括广角、鱼眼或全向相机，都可以模拟为连接单位球体表面的单位射线。假设第l个特征在第i幅图像中被第一次观察到，第j幅图像中的特征观测的残差定义为：<br><img alt="在这里插入图片描述" data-src="https://img-blog.csdnimg.cn/20191019205925902.png#pic_center"><br>其中$[u^{c_i}_l,v^{c_i}_l]$是第一次观测到出现在第i图像中的第l个特征。$[\hat u^{c_i}_l,\hat v^{c_i}_l]$是在第j图像中对相同特征的观察。$π^{−1}_c$​是利用摄像机内参将像素位置转换成单位向量的反投影函数。由于视觉残差的自由度是2，所以我们将残差向量投影到切平面上。如图6所示，b1、b2是在切平面$\hat {\bar P^{c_j}_l}$上的两个任意选择的正交基。我们可以很容易地找到一组$b_1、b_2$，如算法1所示。在(22)中使用的$P^{c_j}_l$​​是正切空间中固定长度的标准协方差。<br><img alt="在这里插入图片描述" data-src="https://img-blog.csdnimg.cn/20191019210021988.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjkwNTE0MQ==,size_16,color_FFFFFF,t_70#pic_center"></p>
<h2 id="D-边缘化"><a href="#D-边缘化" class="headerlink" title="D. 边缘化"></a>D. 边缘化</h2><p>为了限制基于优化的VIO的计算复杂度，本文引入了边缘化。我们有选择地从滑动窗口中将IMU状态xK和特征λ1边缘化，同时将对应于边缘状态的测量值转换为先验。</p>
<p>如图7所示，当倒数第二帧是关键帧时，它将停留在窗口中，而最旧的帧与其相应的测量值被边缘化。但如果倒数第二帧是非关键帧，我们丢掉视觉测量值，保留连接到这个非关键帧的IMU测量值。为了保持系统的稀疏性，我们不会边缘化非关键帧的所有测量值。我们的边缘化方案旨在保持窗口中空间分离的关键帧。这确保了特征三角化有足够的视差，并且最大化了在大激励下获得加速度计测量值的概率。<br><img alt="在这里插入图片描述" data-src="https://img-blog.csdnimg.cn/20191019210401727.png#pic_center"></p>
<h2 id="E-摄像机速率状态估计的纯运动视觉惯性BA"><a href="#E-摄像机速率状态估计的纯运动视觉惯性BA" class="headerlink" title="E. 摄像机速率状态估计的纯运动视觉惯性BA"></a>E. 摄像机速率状态估计的纯运动视觉惯性BA</h2><p>对于计算能力较低的设备如手机，由于对非线性优化的计算要求很高，紧耦合单目VIO无法实现摄像机速率输出。为此，我们采用了一种轻量级的纯运动视觉惯性BA，以提升状态估计速率到相机速率(30Hz)。</p>
<p>纯运动单目视觉惯性BA的代价函数与(22)中单目VIO的代价函数相同。然而我们只对固定数量的最新IMU状态的姿态和速度进行了优化，而不是对滑动窗口中的所有状态进行优化。我们将特征深度、外部参数、偏置和旧的IMU状态这些不希望优化的状态作为常量来处理。我们使用所有的视觉和惯性测量来进行纯运动的BA。这导致了比单帧PnP方法更平滑的状态估计。图8显示了提出方法的插图。与在最先进的嵌入式计算机上可能导致超过50ms的完全紧耦合单目VIO不同，这种纯运动的视觉惯性BA只需大约5ms来计算。这使得低延迟的相机频率进行位姿估计对无人机和AR应用特别有利。<br>F. IMU前向传递以达到IMU速率状态估计</p>
<p>IMU测量的速度远高于视觉测量。虽然我们的VIO频率受到图像捕获频率的限制，但是我们仍然可以通过最近的IMU测量来直接传递最新的VIO估计，以达到IMU速率的性能。高频状态估计可以作为回环检测的状态反馈。利用这种IMU速率状态估计进行的自主飞行实验在第九节的D中给出。</p>
<h2 id="F-IMU前向传递以达到IMU速率状态估计"><a href="#F-IMU前向传递以达到IMU速率状态估计" class="headerlink" title="F. IMU前向传递以达到IMU速率状态估计"></a>F. IMU前向传递以达到IMU速率状态估计</h2><p>IMU测量的速度远高于视觉测量。虽然我们的VIO频率受到图像捕获频率的限制，但是我们仍然可以通过最近的IMU测量来直接传递最新的VIO估计，以达到IMU速率的性能。高频状态估计可以作为回环检测的状态反馈。利用这种IMU速率状态估计进行的自主飞行实验在第九节的D中给出。</p>
<h2 id="G-故障检测与恢复"><a href="#G-故障检测与恢复" class="headerlink" title="G. 故障检测与恢复"></a>G. 故障检测与恢复</h2><p>虽然我们紧耦合的单目视觉对各种具有挑战性的环境和运动是鲁棒的。由于强烈的光照变化或剧烈的运动，故障仍然是不可避免的。主动故障检测和恢复策略可以提高系统的实用性。故障检测是一个独立的模块，它检测估计器的异常输出。我们目前使用以下标准进行故障检测：<br>1、在最新帧中跟踪的特征数小于某一阈值；<br>2、最近两个估计器输出之间的位置或旋转有较大的不连续性；<br>3、偏置或外部参数估计有较大的变化；</p>
<p>一旦检测到故障，系统将切换回初始化阶段。一旦单目VIO被成功初始化，将新建一个独立的位姿图。</p>
<h1 id="VII-重定位"><a href="#VII-重定位" class="headerlink" title="VII. 重定位"></a>VII. 重定位</h1><p>我们的滑动窗口和边缘化方案限制了计算的复杂性，但也给系统带来了累积漂移。更确切地说，漂移发生在全局三维位置(x,y,z)和围绕重力方向的旋转(yaw)。为了消除漂移，提出了一种与单目VIO无缝集成的紧耦合重定位模块。重定位过程从一个循环检测模块开始，该模块识别已经访问过的地方。然后建立回环检测候选帧和当前帧之间的特征级连接。这些特征的对应关系紧密地集成到单目VIO模块中，从而以最小计算代价得到无漂移状态估计。多个特征的多个观测直接用于重定位，从而提高了定位的精度和状态估计的平滑性。重定位过程如图9(a)所示。<br><img alt="在这里插入图片描述" data-src="https://img-blog.csdnimg.cn/20191019210456719.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjkwNTE0MQ==,size_16,color_FFFFFF,t_70#pic_center"></p>
<h2 id="B-特征恢复"><a href="#B-特征恢复" class="headerlink" title="B. 特征恢复"></a>B. 特征恢复</h2><p>当检测到回路时，通过检索特征对应关系建立局部滑动窗口与回环候选帧之间的连接。通过BRIEF描述子匹配找到对应关系。直接描述子匹配可能会造成大量异常值。为此，我们使用两步进行几何异常值剔除，如图10所示。<br>1、2D-2D：RANSAC[31]的基本矩阵检验。我们利用当前图像中检索到的特征的二维观测和回环候选图像进行基本矩阵检验。<br>2、3D-2D：RANSAC的PNP检验。基于特征在局部滑动窗口中已知的三维位置，以及回环候选图像中的二维观测，进行PNP检验。</p>
<p>当内点超过一定阈值时，我们将该候选帧视为正确的循环检测并执行重定位。</p>
<h2 id="C-紧耦合重定位"><a href="#C-紧耦合重定位" class="headerlink" title="C. 紧耦合重定位"></a>C. 紧耦合重定位</h2><p>重定位过程有效地使单目VIO(VI)维持的当前滑动窗口与过去的位姿图对齐。在重定位过程中，我们将所有回环帧的位姿作为常量。利用所有IMU测量值、局部视觉测量和从回环中提取特征对应值，共同优化滑动窗口。我们可以轻松地为回环帧v观察到的检索特征编写视觉测量模型，使其与VIO中的视觉测量相同，如(25)所示。唯一的区别是，从位姿图(VIII)或直接从上一个里程计的输出（如果这是第一次重定位）获得的回环帧的姿态$(\hat q^w_v,\hat p^w_v)$<br>(q^​vw​,p^​vw​)被视为常数。为此，我们可以在(22)中稍微修改非线性代价函数，增加回环项：</p>
<p><img alt="在这里插入图片描述" data-src="https://img-blog.csdnimg.cn/20191019210641949.png#pic_center"><br>其中L是回环帧中检索到的特征的观测集。$(l,v)$是指在回环帧v中观察到的第l个特征。虽然代价函数与(22)略有不同，但待解状态的维数保持不变，因为回环帧的构成被视为常数。当用当前滑动窗口建立多个回环时，我们同时使用来自所有帧的所有回环特征对应进行优化。这就为重定位提供了多视角的约束，从而提高了定位的精度和平滑性。请注意，过去的姿态和回环帧的全局优化发生在重定位之后，将在第八节中讨论。</p>
<h1 id="VIII-全局位姿图优化"><a href="#VIII-全局位姿图优化" class="headerlink" title="VIII. 全局位姿图优化"></a>VIII. 全局位姿图优化</h1><p>重新定位后，局部滑动窗口移动并与过去的位姿对齐。利用重定位结果，开发了额外的位姿图优化步骤，以确保过去位姿集注册到全局一致的配置中。</p>
<p>由于视觉惯性的建立使翻滚角和俯仰角完全可观测，累积漂移只发生在四个自由度(x，y，z和yaw)。为此，我们忽视对无漂移翻滚和俯仰状态的估计，只进行了四自由度位姿图的优化。</p>
<h2 id="A-在位姿图中添加关键帧"><a href="#A-在位姿图中添加关键帧" class="headerlink" title="A. 在位姿图中添加关键帧"></a>A. 在位姿图中添加关键帧</h2><p>当关键帧从滑动窗口被边缘化时，它将被添加到位姿图中。这个关键帧在位姿图中作为顶点，它通过两种类型的边与其他顶点连接：</p>
<p>1）顺序边(Sequential Edge)：关键帧将建立与之前关键帧的若干顺序边。顺序边表示局部滑动窗口中两个关键帧之间的相对转换，其值直接从VIO中获取。考虑到新边缘化的关键帧i及其先前的一个关键帧j，顺序边只包含相对位置和偏航角。<br><img alt="在这里插入图片描述" data-src="https://img-blog.csdnimg.cn/20191019210857102.png#pic_center">)<br>2）回环边(Loop Closure Edge)：如果新边缘化的关键帧有一个回环连接，它将与回环帧通过一个回环边在位姿图图中连接。同样，闭环边缘只包含与(27)相同定义的四自由度相对位姿变换。回环边的值由重定位结果得出。</p>
<h2 id="B-4自由度位姿图优化"><a href="#B-4自由度位姿图优化" class="headerlink" title="B. 4自由度位姿图优化"></a>B. 4自由度位姿图优化</h2><p>我们将帧i和j之间边的残差定义为：<br><img alt="在这里插入图片描述" data-src="https://img-blog.csdnimg.cn/20191019210815393.png#pic_center"><br>其中，$\hat \phi _i，\hat    θ_i$是直接从单目VIO中得到的翻滚角和俯仰角的估计。</p>
<p>通过最小化以下代价函数，对顺序边和回环边的整个图进行优化：<br><img alt="在这里插入图片描述" data-src="https://img-blog.csdnimg.cn/2019101921101530.png#pic_center"></p>
<p>其中S是所有顺序边的集合，L是回环边的集合。尽管紧耦合的重定位已经有助于消除错误的回环，但我们添加了另一个Huber范数 $ρ(·)$，以进一步减少任何可能的错误回环的影响。相反，我们不对顺序边使用任何鲁棒范数，因为这些边是从VIO中提取出来的，VIO已经包含了足够多的外点排除机制。</p>
<p>位姿图优化和重定位(VII-C)异步运行在两个独立的线程中。以便在需要重定位时，能立即使用最优化的位姿图。同样，即使当前的位姿图优化尚未完成，仍然可以使用现有的位姿图配置进行重新定位。这一过程如图9(b)所示。</p>
<h2 id="C-位姿图管理"><a href="#C-位姿图管理" class="headerlink" title="C. 位姿图管理"></a>C. 位姿图管理</h2><p>随着行程距离的增加，位姿图的大小可能会无限增长，从而限制了长时间系统的实时性。为此，我们实行了一个下采样过程，将位姿图数据库保持在有限的大小。所有具有回环约束的关键帧都将被保留，而其他与相邻帧过近或方向非常相似的关键帧可能会被删除。关键帧被移除的概率和其相邻帧的空间密度成正比。</p>
<h1 id="IX-实验结果"><a href="#IX-实验结果" class="headerlink" title="IX. 实验结果"></a>IX. 实验结果</h1><p>我们进行了三个实验和两个应用，以评估所提出的VINS-Mono系统。在第一个实验中，我们将提出的算法与另一种最先进算法在公共数据集上进行比较。我们通过数值分析以验证了系统的精度。然后在室内环境中测试我们的系统，以评估在重复场景中的性能。通过大量的实验验证了系统的长期实用性。此外，我们还将所提出的系统应用于两个应用。对于空中机器人的应用，我们使用VINS-Mono作为位置反馈来控制无人机跟踪预定的轨迹。然后我们将我们的方法移植到iOS移动设备上，并与Google Tango进行比较。</p>
<h2 id="A-数据集比较"><a href="#A-数据集比较" class="headerlink" title="A. 数据集比较"></a>A. 数据集比较</h2><p>我们使用EuRoC MAV视觉-惯性数据集[41]评估我们提出的VINS-Mono。这个数据集是在一架微型飞行器上收集的，它包含立体图像(Aptina MT9V034全局快门、WVGA单色、20 FPS)、同步IMU测量(ADIS 16448、200 Hz)和地面真实状态(Vicon和Leica MS 50)。我们只使用左边相机的图像。在这数据集中会观察到较大的IMU偏置和光照变化。</p>
<p>在这些实验中，我们将VINS-Mono和OKVIS进行了比较，这是一种最先进的单目和立体相机VIO。OKVIS是另一种基于优化的滑动窗口算法。我们的算法与OKVIS在许多细节上是不同的，如技术部分所示。我们的系统具有良好的初始化和回环功能。我们使用MH_03_median和MH_05_difficult两组序列来证明该方法的性能。为了简化表示，我们使用VINS来表示我们只使用单目VIO的方法，而VINS_loop表示含重定位和位姿图优化的完全版本。我们分别用OKVIS_Mono和OKVIS_stereo表示OKVIS使用单目和立体图像的结果。为了进行公平的比较，我们丢弃前100个输出，并使用接下来的150个输出对齐地面真值，并比较其余的估计器输出。</p>
<p>MH_03_median序列轨迹如图11所示。我们只比较平移误差，因为旋转运动在这个序列中是可以忽略的。图12显示了x、y、z误差与时间的关系，以及平移误差与距离的关系。在误差图中，具有回环的VINS-Mono具有最小的平移误差。我们在MH_05_difficult上观察到类似的结果。该方法具有最小的平移误差。平移和旋转误差如图14所示。由于该序列运动平稳，偏角变化不大，只发生位置漂移。显然，回环闭合有效地约束了累积漂移。OKVIS在翻滚角和俯仰角估计方面表现更好。一个可能的原因是VINS-Mono采用了预积分技术，即IMU传递的一阶近似，以节省计算资源。</p>
<p>VINS-Mono在所有Euroc数据集中都表现良好，即使在最具挑战性的序列V1_03_difficult中，它具有剧烈性的运动、纹理较少的区域和显著的光照变化。由于采用了专用的初始化过程，该方法可以在V1_03_difficult快速初始化。</p>
<p>对于纯VIO，VINS-Mono和OKVIS具有相似的精度，很难区分哪个比较好。然而，VINS-Mono在系统级别上优于OKVIS。它是一个完整的系统，具有鲁棒的初始化和回环闭合功能来辅助单目视觉。</p>
<p><img alt="在这里插入图片描述" data-src="https://img-blog.csdnimg.cn/20191019211301627.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjkwNTE0MQ==,size_16,color_FFFFFF,t_70#pic_center"><br><img alt="在这里插入图片描述" data-src="https://img-blog.csdnimg.cn/20191019211334529.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjkwNTE0MQ==,size_16,color_FFFFFF,t_70#pic_center"></p>
<h2 id="B-室内实验"><a href="#B-室内实验" class="headerlink" title="B. 室内实验"></a>B. 室内实验</h2><p>在室内实验中，我们选择实验室环境作为实验区域。我们使用的传感器套件如图15所示。它搭载在DJI A3控制器上，包含一个单目照相机（20Hz）和一个IMU（100 Hz）。我们手握传感器套件，在实验室以正常的速度行走。如图16所示，我们遇到行人，光线较弱的位置，纹理较少的区域，玻璃和反射。多媒体附件中可以找到视频。 </p>
<p><img alt="在这里插入图片描述" data-src="https://img-blog.csdnimg.cn/20191019211501205.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjkwNTE0MQ==,size_16,color_FFFFFF,t_70#pic_center"><br>我们将我们的结果与OKVIS进行了比较，如图17所示。图17(a)是OKVIS的VIO输出。图17(b)是所提出的无回环方法的VIO结果。图17©是所提出的具有重定位和回环闭合的方法的结果。当我们在室内转圈时，会出现明显的漂移。OKVIS和只有VIO版本的VINS-Mono在x、y、z和偏航角上积累了大量漂移。我们的重定位和回环闭合模块有效地消除了所有这些漂移。<br><img alt="在这里插入图片描述" data-src="https://img-blog.csdnimg.cn/20191019211551914.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjkwNTE0MQ==,size_16,color_FFFFFF,t_70#pic_center"></p>
<h2 id="C-大范围环境"><a href="#C-大范围环境" class="headerlink" title="C. 大范围环境"></a>C. 大范围环境</h2><p>1）走出实验室：我们在室内和室外混合环境中测试VINS-Mono。传感器套件与图15所示的相同。我们从实验室的一个座位上开始，在室内空间里走来走去。然后我们下楼，在大楼外的操场上走来走去。接下来，我们回到楼里并上楼。最后，我们回到了实验室的同一个座位。整个轨迹超过700米，持续约10分钟。在多媒体附件中可以找到实验的视频。</p>
<p>轨迹如图19所示。图19(a)是OKVIS的轨迹。当我们上楼时，OKVIS显示出不稳定的特征跟踪，导致估计错误。我们看不到红色街区楼梯的形状。VINS-Mono的纯VIO结果如图19(b)所示。有闭环的轨迹如图19©所示。该方法的楼梯形状清晰。为验证其准确性，将闭环轨迹与谷歌地图对齐，如图18所示。<br><img alt="在这里插入图片描述" data-src="https://img-blog.csdnimg.cn/2019101921164349.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjkwNTE0MQ==,size_16,color_FFFFFF,t_70#pic_center"><br>OKVIS的 x、y和z轴的最终漂移为[13.80,-5.26,7.23]米。VINS-Mono无环闭路的最终漂移为[-5.47,2.76,-0.29]m，占整个轨迹长度的0.88%，小于OKVIS的2.36%。经回环修正，最终漂移上界为[-0.032,0.09,-0.07]m，与整个轨迹长度相比这是微不足道的。虽然我们没有地面真值，但我们仍然可以直观地检查优化后的轨迹是否平滑并能精确地与卫星地图对齐。</p>
<p>2）环游校园：这张环绕整个科大校园的非常大规模的数据集是用一个手持的VI-Sensor 4记录下来的。该数据集覆盖的地面长度约为710米，宽度为240米，高度变化为60米。总路径长度为5.62km。数据包含25Hz图像和200Hz IMU，持续1小时34分钟。对VINS-Mono的稳定性和耐久性进行测试是一个非常有意义的实验。</p>
<p>在这个大规模的测试中，我们将关键帧数据库的大小设置为2000，以提供足够的回环信息并达到实时性。我们运行此数据集时，采用英特尔i7-4790 CPU运行在3.60GHz。时间统计数据显示在表I中。如图20，估计的轨迹与谷歌地图一致。与谷歌地图相比，我们的结果在这个非常长时间的测试中几乎没有漂移。<br><img alt="在这里插入图片描述" data-src="https://img-blog.csdnimg.cn/2019101921174092.png#pic_center"><br><img alt="在这里插入图片描述" data-src="https://img-blog.csdnimg.cn/20191019211751315.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjkwNTE0MQ==,size_16,color_FFFFFF,t_70#pic_center"><br>D. 应用1：航空机器人的反馈控制</p>
<p>如图21(a)所示，我们将VINS-Mono应用于航空机器人的自主反馈控制。我们使用了一个具有752×480分辨率的前向全局曝光相机(MatrixVisionMvBlueFOXMLC200w)，并配备了190度鱼眼镜头。DJIA3飞行控制器用于IMU测量和姿态稳定控制。机载计算资源是Intel i7-5500 U CPU运行在3.00GHz。传统的针孔摄像机模型不适用于大视场摄像机。我们使用MEI[42]模型，由[43]介绍的工具包进行校准。</p>
<p><img alt="在这里插入图片描述" data-src="https://img-blog.csdnimg.cn/20191019211839510.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjkwNTE0MQ==,size_16,color_FFFFFF,t_70#pic_center"><br>在本实验中，我们测试使用VINS_Mono的状态估计来进行自主轨迹跟踪的性能。实验中回环检测被禁止。四旋翼被命令跟踪一个八字形图案，每个圆圈半径为1.0米，如图21(b)所示。在轨迹周围设置了四个障碍物，以验证VINS-Mono无闭环的准确性。在实验过程中，四旋翼连续四次跟踪这一轨迹。100 Hz机载状态估计(VI-F)支持对四旋翼的实时反馈控制。</p>
<p>地面真值是用OptiTrack 5获得的。总轨迹长度为61.97 m。最终漂移为[0.08，0.09，0.13]m，为0.29%的位置漂移。平移和旋转的细节以及它们相应的误差如图23所示。<br><img alt="在这里插入图片描述" data-src="https://img-blog.csdnimg.cn/20191019211919318.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjkwNTE0MQ==,size_16,color_FFFFFF,t_70#pic_center"></p>
<h2 id="E-应用2：移动设备"><a href="#E-应用2：移动设备" class="headerlink" title="E. 应用2：移动设备"></a>E. 应用2：移动设备</h2><p>我们将VINS-Mono移植到移动设备上，并提供一个简单的AR应用程序来展示其准确性和鲁棒性。我们将我们的移动实现命名为VINS-Mobile6，并将其与Google Tango Device 7进行了比较，后者是移动平台在商业上最好的增强现实解决方案之一。</p>
<p>VINS-Mono运行在iPhone7 Plus上。我们使用iPhone采集的30 Hz、分辨率为640×480的图像，以及内置InvenSense MP67B 6轴陀螺仪和加速度计获得的100 Hz 的IMU数据。如图24所示，我们将iPhone与一个启用Tango功能的联想Phab 2 Pro一起安装。Tango设备使用全局快门、鱼眼相机和同步IMU进行状态估计。首先，我们在从估计的视觉特征中提取出来的平面上插入一个虚拟立方体，如图25(a)所示。然后，我们拿着这两个装置，以正常的速度在房间内外行走。当检测到回环时，我们使用四自由度位姿图优化(VIII-B)，以消除x，y，z和yaw漂移。<br><img alt="在这里插入图片描述" data-src="https://img-blog.csdnimg.cn/20191019212019875.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjkwNTE0MQ==,size_16,color_FFFFFF,t_70"><br><img alt="在这里插入图片描述" data-src="https://img-blog.csdnimg.cn/20191019212037844.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjkwNTE0MQ==,size_16,color_FFFFFF,t_70#pic_center"><br><img alt="在这里插入图片描述" data-src="https://img-blog.csdnimg.cn/2019101921210026.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MjkwNTE0MQ==,size_16,color_FFFFFF,t_70#pic_center"><br>有趣的是，当我们打开一扇门时，Tango的偏航角估计会跳转到一个很大的角度，如图25(b)所示。其原因可能是由于不稳定的特征跟踪或主动故障检测和恢复而导致的估计器崩溃。然而，VINS-Mono在这个具有挑战性的情况中仍然表现很好。行走了大约264米后我们回到起点。最后的结果可以在图25( c)中看到，Tango的轨迹在最后一圈会漂移，而我们的VINS会回到起点。四自由度位姿图的优化消除了总轨迹的漂移。这与开始相比，立方体被标记到图像上的同一位置也印证了这一点。</p>
<p>诚然，尤其是对局部状态的估计，Tango比我们的实现更准确。但是实验结果表明，我们的方法可以在通用移动设备上运行，并且具有媲美特殊工程设备的潜力。实验还证明了该方法的鲁棒性。视频可以在多媒体附件中找到。</p>
<h1 id="X-结论和未来工作"><a href="#X-结论和未来工作" class="headerlink" title="X. 结论和未来工作"></a>X. 结论和未来工作</h1><p>本文提出了一种鲁棒、通用的单目视觉惯性估计器。我们的方法在IMU预积分，估计器初始化和故障恢复，在线外部校准，紧耦合视觉惯性里程计，重定位和有效的全局优化上，具有最先进的和新颖的解决方案。我们通过与最先进的开源实现和高度优化的行业解决方案进行比较，显示出更好的性能。我们开源了PC和iOS的实现，以造福社会。</p>
<p>虽然基于特征的VINS估计器已经达到了实际使用的成熟程度，我们仍然看到了未来研究的许多方向。单目VINS可能会根据运动和环境而达到较难观测甚至退化的状态。我们最感兴趣的是在线方法来评估单目VINS的可观测性，以及在线生成运动计划来恢复可观测性。另一个研究方向是在大量消费设备上大规模部署单目VINS，例如移动电话。这一应用要求在线校准几乎所有传感器的内参和外参，以及在线鉴定校准质量。最后，我们感兴趣的是制作由单目VINS给出的稠密地图。我们在[44]中首次给出了用于无人机导航的单目视觉-惯性稠密地图的结果。然而，仍需进行广泛的研究以进一步提高系统的精度和鲁棒性。</p>
]]></content>
      <categories>
        <category>VINS</category>
      </categories>
      <tags>
        <tag>VINS-Mono</tag>
        <tag>VSLAM</tag>
      </tags>
  </entry>
  <entry>
    <title>VINS-Mono论文学习与代码解读</title>
    <url>/2019/10/18/VINS-Mono%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0%E4%B8%8E%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><ol>
<li><a href="https://blog.csdn.net/qq_41839222/article/details/86564879" target="_blank" rel="noopener">VINS-Mono代码解读——启动文件launch与参数配置文件yaml介绍</a></li>
<li><a href="https://blog.csdn.net/qq_41839222/article/details/86030962" target="_blank" rel="noopener">VINS-Mono代码解读——各种数据结构 sensor_msgs</a></li>
<li><a href="https://blog.csdn.net/qq_41839222/article/details/85797156" target="_blank" rel="noopener">VINS-Mono代码解读——视觉跟踪 feature_trackers</a></li>
<li><a href="https://blog.csdn.net/qq_41839222/article/details/86290941" target="_blank" rel="noopener">VINS-Mono理论学习——IMU预积分 Pre-integration （Jacobian 协方差）</a></li>
<li><a href="https://blog.csdn.net/qq_41839222/article/details/86293038" target="_blank" rel="noopener">VINS-Mono代码解读——状态估计器流程 estimator 写在初始化和非线性优化前</a></li>
<li><a href="https://blog.csdn.net/qq_41839222/article/details/88942414" target="_blank" rel="noopener">VINS-Mono代码解读——视觉惯性联合初始化 initialStructure sfm</a></li>
<li><a href="https://blog.csdn.net/qq_41839222/article/details/89106128" target="_blank" rel="noopener">VINS-Mono理论学习——视觉惯性联合初始化与外参标定</a></li>
<li><a href="https://blog.csdn.net/qq_41839222/article/details/93593844" target="_blank" rel="noopener">VINS-Mono理论学习——后端非线性优化</a></li>
<li><a href="https://blog.csdn.net/qq_41839222/article/details/87878550" target="_blank" rel="noopener">VINS-Mono代码解读——回环检测与重定位 pose graph loop closing</a></li>
</ol>]]></content>
      <categories>
        <category>VIO</category>
      </categories>
  </entry>
</search>
